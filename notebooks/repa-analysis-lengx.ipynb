{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866b26dd-0628-452c-9657-e40212efb23d",
   "metadata": {},
   "source": [
    "# 0 - Quick Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2f783-e5ec-4f25-a7f9-f36e43767212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "os.path.join(os.path.defpath)\n",
    "import os\n",
    "\n",
    "# Construct the path relative to home\n",
    "project_path = os.path.join(os.path.expanduser('~'), 'project', 'grepa', 'REPA')\n",
    "# project_path = \"./\"\n",
    "# Change the current working directory\n",
    "os.chdir(project_path)\n",
    "print(os.getcwd()) # confirm the working directory is the repo root\n",
    "\n",
    "\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30025170-3aec-49a4-991a-eda0d940598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_alongside_batch(img_list, resize_dims=(512,512)):\n",
    "    if isinstance(resize_dims, int):\n",
    "        resize_dims = (resize_dims,resize_dims)\n",
    "    res = np.concatenate([np.array(img.resize(resize_dims)) for img in img_list], axis=1)\n",
    "    return Image.fromarray(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4e3d8-5cab-425b-bcdb-d6f3c0ef3fda",
   "metadata": {},
   "source": [
    "# 1 - Load the model (SiT) and encoder (DINO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9b6331-9195-4332-b389-b83c21c7506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --model SiT-XL/2 \\\n",
    "#   --num-fid-samples 50000 \\\n",
    "#   --ckpt YOUR_CHECKPOINT_PATH \\\n",
    "#   --path-type=linear \\\n",
    "#   --encoder-depth=8 \\\n",
    "#   --projector-embed-dims=768 \\\n",
    "#   --per-proc-batch-size=64 \\\n",
    "#   --mode=sde \\\n",
    "#   --num-steps=250 \\\n",
    "#   --cfg-scale=1.8 \\\n",
    "#   --guidance-high=0.7\n",
    "\n",
    "# Load model:\n",
    "from dataclasses import dataclass\n",
    "from models.sit import SiT_models\n",
    "from models.original_sit import SiT_models as Original_SiT_Model\n",
    "from diffusers.models import AutoencoderKL\n",
    "\n",
    "# NOTE: Experiment for the REPA checkpoints\n",
    "@dataclass\n",
    "class repaArgs():\n",
    "    vae = 'ema'\n",
    "    model =  'SiT-XL/2' #'SiT-B/2' #'SiT-XL/2'\n",
    "    num_classes = 1000\n",
    "    encoder_depth = 8\n",
    "    projector_embed_dims = \"768\"\n",
    "    fused_attn = True\n",
    "    qk_norm = False\n",
    "    resolution = 256\n",
    "    # ckpt = './exps/sit-b-base-400k/checkpoints/0250000.pt'\n",
    "    # ckpt = './exps/sit-b-linear-dinov2-b-enc8-400k/checkpoints/0200000.pt'\n",
    "    ckpt = './pretrained_models/last.pt'\n",
    "    # ckpt = './pretrained_models/SiT-XL-2-256x256-fixed.pt'\n",
    "    data_dir = './data/'\n",
    "\n",
    "args = repaArgs()\n",
    "device = \"cuda:0\"\n",
    "\n",
    "block_kwargs = {\"fused_attn\": args.fused_attn, \"qk_norm\": args.qk_norm}\n",
    "latent_size = args.resolution // 8\n",
    "model = SiT_models[args.model](\n",
    "    input_size=latent_size,\n",
    "    num_classes=args.num_classes,\n",
    "    use_cfg = True,\n",
    "    z_dims = [int(z_dim) for z_dim in args.projector_embed_dims.split(',')],\n",
    "    encoder_depth=args.encoder_depth,\n",
    "    **block_kwargs,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Auto-download a pre-trained model or load a custom SiT checkpoint from train.py:\n",
    "ckpt_path = args.ckpt\n",
    "state_dict = torch.load(ckpt_path, map_location=f'{device}')#['ema']\n",
    "if \"model\" in state_dict:\n",
    "    state_dict = state_dict[\"model\"]\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  # important!\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{args.vae}\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347a891b-6421-49f8-ae42-32cc0355c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from samplers import euler_sampler, euler_maruyama_sampler\n",
    "# Ensure your model and vae are already loaded as in your snippet\n",
    "from utils.imagenet_idx_to_classnames import imagenet_idx_to_classnames\n",
    "\n",
    "def sample_images(\n",
    "    model, \n",
    "    vae, \n",
    "    class_idx=None,\n",
    "    batch_size=8,\n",
    "    resolution=256, \n",
    "    num_steps=50, \n",
    "    mode='ode', \n",
    "    cfg_scale=1.5, \n",
    "    guidance_low=0.0, \n",
    "    guidance_high=1.0, \n",
    "    path_type='linear', \n",
    "    num_classes=1000, \n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample images from a SiT model in a simple single-GPU environment.\n",
    "    \n",
    "    Parameters:\n",
    "        model: The loaded SiT model.\n",
    "        vae: The loaded VAE model from diffusers.\n",
    "        batch_size (int): Number of images to sample in one batch.\n",
    "        resolution (int): The output resolution of images.\n",
    "        num_steps (int): Number of diffusion steps (ODE or SDE steps).\n",
    "        mode (str): 'ode' or 'sde' sampling mode.\n",
    "        cfg_scale (float): Classifier-free guidance scale.\n",
    "        guidance_low (float): Guidance lower bound.\n",
    "        guidance_high (float): Guidance upper bound.\n",
    "        path_type (str): 'linear' or 'cosine' for scheduling.\n",
    "        num_classes (int): Number of classes for the model.\n",
    "        device (str): Device to run the sampling on.\n",
    "        \n",
    "    Returns:\n",
    "        A list of PIL.Image objects.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    latent_size = resolution // 8\n",
    "\n",
    "    # Sample latent noise\n",
    "    z = torch.randn(batch_size, model.in_channels, latent_size, latent_size, device=device)\n",
    "    if class_idx:\n",
    "        y = torch.tensor([class_idx] * batch_size, device=device)\n",
    "    else:\n",
    "        # Sample random class labels (if using an ImageNet-like model)\n",
    "        y = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "        \n",
    "    # Prepare sampler arguments\n",
    "    sampling_kwargs = dict(\n",
    "        model=model,\n",
    "        latents=z,\n",
    "        y=y, # label condition\n",
    "        num_steps=num_steps,\n",
    "        heun=False,  # Set True if you want Heun steps (for ODE sampler)\n",
    "        cfg_scale=cfg_scale,\n",
    "        guidance_low=guidance_low,\n",
    "        guidance_high=guidance_high,\n",
    "        path_type=path_type,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pick the sampler\n",
    "        if mode == \"sde\":\n",
    "            samples = euler_maruyama_sampler(**sampling_kwargs).to(torch.float32)\n",
    "        elif mode == \"ode\":\n",
    "            samples = euler_sampler(**sampling_kwargs).to(torch.float32)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Mode must be 'ode' or 'sde'.\")\n",
    "\n",
    "        # Decode with VAE\n",
    "        latents_scale = torch.tensor([0.18215]*4).view(1,4,1,1).to(device)\n",
    "        latents_bias = torch.tensor([0.0]*4).view(1,4,1,1).to(device)\n",
    "        decoded = vae.decode((samples - latents_bias) / latents_scale).sample\n",
    "        decoded = (decoded + 1) / 2.0\n",
    "        decoded = torch.clamp(255. * decoded, 0, 255).permute(0,2,3,1).to(\"cpu\", dtype=torch.uint8).numpy()\n",
    "\n",
    "    class_idxs = y.detach().cpu().numpy()\n",
    "    # Convert to PIL images\n",
    "    images = [Image.fromarray(decoded[i]) for i in range(decoded.shape[0])]\n",
    "    return images, class_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0250c-a732-4005-97ca-ce3866af7e6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.imagenet_idx_to_classnames import imagenet_idx_to_classnames\n",
    "\n",
    "# generate samples\n",
    "device = \"cuda:0\"  # or \"cpu\"\n",
    "class_idx = None #2 #None\n",
    "\n",
    "images, class_idxs = sample_images(\n",
    "    model=model, \n",
    "    vae=vae,\n",
    "    class_idx=class_idx,\n",
    "    batch_size=4, \n",
    "    resolution=256, \n",
    "    num_steps=100, \n",
    "    mode='ode', \n",
    "    cfg_scale=2.5, \n",
    "    guidance_low=0.0, \n",
    "    guidance_high=1.0, \n",
    "    path_type='linear', \n",
    "    num_classes=1000,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "class_names = [imagenet_idx_to_classnames[x] for x in class_idxs]\n",
    "if class_idx is None:\n",
    "    # Display the sampled images directly in the notebook\n",
    "    for img, class_name in zip(images, class_names):\n",
    "        print (class_name)\n",
    "        display(img)\n",
    "\n",
    "print (class_names)\n",
    "display_alongside_batch(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17da9e0-2139-4cb4-ab8a-0661fd870b1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imagenet_idx_to_classnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996c05a-0128-417e-ad94-838957f88689",
   "metadata": {},
   "source": [
    "## 1.2 - Extract DINO and Diffusion features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c771fe0-6a7a-4010-a444-fa2c39958fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "@dataclass\n",
    "class repaArgs:\n",
    "    # Logging and I/O\n",
    "    output_dir: str = \"exps\"\n",
    "    exp_name: str = \"my_experiment\"  # originally required; provide a default\n",
    "    logging_dir: str = \"logs\"\n",
    "    report_to: str = \"wandb\"\n",
    "    sampling_steps: int = 10000\n",
    "    resume_step: int = 0\n",
    "\n",
    "    # Model\n",
    "    model: str = \"SiT-XL/2\"\n",
    "    num_classes: int = 1000\n",
    "    # encoder_depth: int = 8\n",
    "    encoder_depth: int = 20\n",
    "    fused_attn: bool = True\n",
    "    qk_norm: bool = False\n",
    "\n",
    "    # Dataset\n",
    "    data_dir: str = \"./data/\"\n",
    "    resolution: int = 256\n",
    "    batch_size: int = 256\n",
    "\n",
    "    # Precision\n",
    "    allow_tf32: bool = False\n",
    "    mixed_precision: Literal[\"no\", \"fp16\", \"bf16\"] = \"fp16\"\n",
    "\n",
    "    # Optimization\n",
    "    epochs: int = 1400\n",
    "    max_train_steps: int = 400000\n",
    "    checkpointing_steps: int = 40000\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    learning_rate: float = 1e-4\n",
    "    adam_beta1: float = 0.9\n",
    "    adam_beta2: float = 0.999\n",
    "    adam_weight_decay: float = 0.0\n",
    "    adam_epsilon: float = 1e-08\n",
    "    max_grad_norm: float = 1.0\n",
    "\n",
    "    # Seed\n",
    "    seed: int = 0\n",
    "\n",
    "    # CPU-related\n",
    "    num_workers: int = 4\n",
    "\n",
    "    # Loss\n",
    "    path_type: Literal[\"linear\", \"cosine\"] = \"linear\"\n",
    "    prediction: Literal[\"v\"] = \"v\"  # currently only 'v' supported\n",
    "    cfg_prob: float = 0.1\n",
    "    enc_type: str = \"dinov2-vit-b\"\n",
    "    proj_coeff: float = 0.5\n",
    "    weighting: str = \"uniform\"\n",
    "    legacy: bool = False\n",
    "\n",
    "    # Additional model/vae params from your initial snippet\n",
    "    vae: str = \"ema\"\n",
    "    projector_embed_dims: str = \"768\"\n",
    "    ckpt: str = \"./pretrained_models/last.pt\"\n",
    "\n",
    "# train dataset\n",
    "from dataset import CustomDataset, CustomZipDataset, CustomH5Dataset\n",
    "\n",
    "def load_train_dataset(args):\n",
    "    if (os.path.exists(os.path.join(args.data_dir, \"images\")) and\n",
    "        os.path.exists(os.path.join(args.data_dir, \"vae-sd\"))):\n",
    "        train_dataset = CustomDataset(args.data_dir)\n",
    "    elif (os.path.exists(os.path.join(args.data_dir, \"images.h5\")) and\n",
    "          os.path.exists(os.path.join(args.data_dir, \"vae-sd.h5\")) and\n",
    "          os.path.exists(os.path.join(args.data_dir, \"images_h5.json\")) and\n",
    "          os.path.exists(os.path.join(args.data_dir, \"vae-sd_h5.json\"))):\n",
    "            train_dataset = CustomH5Dataset(args.data_dir)\n",
    "    elif (os.path.exists(os.path.join(args.data_dir, \"images.zip\")) and\n",
    "          os.path.exists(os.path.join(args.data_dir, \"vae-sd.zip\"))):\n",
    "        train_dataset = CustomZipDataset(args.data_dir)\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not found.\")\n",
    "    return train_dataset\n",
    "\n",
    "from utils import load_encoders\n",
    "def load_perception_encoders(args):\n",
    "    if args.enc_type != 'None':\n",
    "        encoders, encoder_types, architectures = load_encoders(args.enc_type, device)\n",
    "    else:\n",
    "        encoders, encoder_types, architectures = [None], [None], [None]\n",
    "    return encoders, encoder_types, architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160267f-509c-4147-b7c0-cc2cb9ca1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load args and training dataset\n",
    "args = repaArgs()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec63ba-4061-4abf-a94f-e6a110855946",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.enc_type, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c699fa-a099-421d-9c20-63f6ae2dc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import SILoss\n",
    "\n",
    "# get training dataset, encoders and loss_fn\n",
    "train_dataset = load_train_dataset(args)  # Uses args.data_dir\n",
    "encoders, encoder_types, architectures = load_perception_encoders(args)\n",
    "\n",
    "def get_loss_fn(args):\n",
    "    # create loss function\n",
    "    loss_fn = SILoss(\n",
    "        prediction=args.prediction,\n",
    "        path_type=args.path_type, \n",
    "        encoders=encoders,\n",
    "        accelerator=None, # since we only need the interpolation fn\n",
    "        latents_scale=None,# since we only need the interpolation fn\n",
    "        latents_bias=None,# since we only need the interpolation fn\n",
    "        weighting=args.weighting\n",
    "    )\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "loss_fn = get_loss_fn(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45b37a88-c533-496f-9aeb-ac472ee442e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from train import sample_posterior, preprocess_raw_image\n",
    "\n",
    "# Model should be in eval mode if we're just extracting features\n",
    "model.eval()\n",
    "\n",
    "def get_batch_features(\n",
    "    model, \n",
    "    vae, \n",
    "    encoders, \n",
    "    encoder_types, \n",
    "    architectures, \n",
    "    train_dataset, \n",
    "    loss_fn,\n",
    "    device='cuda', \n",
    "    batch_size=8,\n",
    "    use_projection=True, \n",
    "    t_start=0.,\n",
    "    t_end=1.,\n",
    "):\n",
    "    # Setup scale/bias for latents\n",
    "    latents_scale = torch.tensor([0.18215]*4).view(1,4,1,1).to(device)\n",
    "    latents_bias = torch.tensor([0.0]*4).view(1,4,1,1).to(device)\n",
    "\n",
    "    # Get one batch\n",
    "    temp_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    raw_image, x, y = next(iter(temp_loader))\n",
    "    raw_image = raw_image.to(device)\n",
    "    x = x.squeeze(dim=1).to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # If needed, handle cfg_prob / legacy label dropping here. For simplicity:\n",
    "    labels = y\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert VAE latents to model latents\n",
    "        x = sample_posterior(x, latents_scale=latents_scale, latents_bias=latents_bias)\n",
    "\n",
    "        # Extract encoder features\n",
    "        zs = []\n",
    "        for encoder, encoder_type, arch in zip(encoders, encoder_types, architectures):\n",
    "            raw_image_ = preprocess_raw_image(raw_image, encoder_type)\n",
    "            z = encoder.forward_features(raw_image_)\n",
    "            if 'mocov3' in encoder_type:\n",
    "                z = z[:, 1:]\n",
    "            if 'dinov2' in encoder_type:\n",
    "                z = z['x_norm_patchtokens']\n",
    "            zs.append(z)\n",
    "\n",
    "        # Sample a random time step (uniform weighting)\n",
    "        time_input = torch.rand((x.shape[0], 1, 1, 1), device=device, dtype=x.dtype)\n",
    "        # limit to the given range\n",
    "        time_input = time_input * (t_end - t_start) + t_start\n",
    "        \n",
    "        alpha_t, sigma_t, d_alpha_t, d_sigma_t = loss_fn.interpolant(time_input)\n",
    "        noises = torch.randn_like(x)\n",
    "        model_input = alpha_t * x + sigma_t * noises\n",
    "        model_target = d_alpha_t * x + d_sigma_t * noises\n",
    "\n",
    "        # Forward pass through the model to get zs_tilde and predictions\n",
    "        model_output, zs_tilde, _, _ = model(model_input, time_input.flatten(), y=labels, use_projection=use_projection)\n",
    "        # model_output, zs_tilde = model(model_input, time_input.flatten(), y=labels, use_projection=use_projection)\n",
    "\n",
    "    return {\n",
    "        \"raw_image\": raw_image,\n",
    "        \"model_input\": model_input,\n",
    "        \"zs\": zs,\n",
    "        \"zs_tilde\": zs_tilde,\n",
    "        \"model_output\": model_output,\n",
    "        \"time_input\": time_input,\n",
    "        \"model_target\": model_target,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d7a5130-482d-4f38-ab55-64c6ec02230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage in a Jupyter cell:\n",
    "model.encoder_depth = args.encoder_depth\n",
    "\n",
    "total_bs = 256\n",
    "bs = 256\n",
    "features = {}\n",
    "t_start = 0.499\n",
    "t_end = 0.501\n",
    "\n",
    "\n",
    "for _ in range(total_bs // bs):\n",
    "    feats = get_batch_features(\n",
    "        model=model,\n",
    "        vae=vae,\n",
    "        encoders=encoders,\n",
    "        encoder_types=encoder_types,\n",
    "        architectures=architectures,\n",
    "        train_dataset=train_dataset,\n",
    "        use_projection=False,  # <-- we shouldn't use the projection head here...\n",
    "        loss_fn=loss_fn,\n",
    "        device=device,\n",
    "        batch_size=bs,\n",
    "        t_start=t_start,\n",
    "        t_end=t_end,\n",
    "    )\n",
    "    for k, v in feats.items():\n",
    "        if k not in features:\n",
    "            features[k] = []\n",
    "        # We should be only using one encoder, safely take the first element\n",
    "        if k in [\"zs\", \"zs_tilde\"]:\n",
    "            features[k].append(feats[k][0])\n",
    "        else:\n",
    "            features[k].append(v)\n",
    "\n",
    "for k, v in features.items():\n",
    "    features[k] = torch.cat(v, dim=0)\n",
    "    # Group them back as a list...\n",
    "    if k in [\"zs\", \"zs_tilde\"]:\n",
    "        features[k] = [features[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28ab902-423f-4a5d-bc50-0d5c99e8e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing the requested details nicely\n",
    "# print(\"Keys in features:\\n\", list(features.keys()), \"\\n\")\n",
    "\n",
    "# print(f\"Shape of raw_image:\\n{features['raw_image'].shape}\\n\")\n",
    "\n",
    "# print(\"Shapes of model_input, model_output, model_target:\\n\", f\"{features['model_input'].shape}, {features['model_output'].shape}, {features['model_target'].shape}\\n\")\n",
    "\n",
    "# print(f\"Lengths of zs and zs_tilde:\\n{len(features['zs'])}, {len(features['zs_tilde'])}\\n\")\n",
    "\n",
    "# print(\"Shapes of first elements in zs and zs_tilde:\\n\", \n",
    "#       f\"{features['zs'][0].shape}, {features['zs_tilde'][0].shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c99f50-8b9e-44ef-b2ec-bd8ad0ca2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348dc958-572f-4be3-b0dd-1c252820a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['raw_image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6e020-552d-43b6-b4a3-fb88be29de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae style features but for different timesteps\n",
    "features['model_input'].shape, features['model_output'].shape, features['model_target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb84ef-b2b6-4edf-ad40-0ea1e401bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features['zs']), len(features['zs_tilde'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf382859-0375-4475-a155-8f540f3c1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder (dino) and  diffusion features (after projection)\n",
    "features['zs'][0].shape, features['zs_tilde'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fdb4fa-2aae-4692-ac3c-ce37868bbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['time_input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5bee8-1447-4fd2-9806-c3aa7d2b2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignment score\n",
    "from metrics import AlignmentMetrics\n",
    "feat_A = features['zs'][0].mean(dim=1) # B, D\n",
    "feat_B = features['zs_tilde'][0].mean(dim=1) # # B, D\n",
    "alignment_score = AlignmentMetrics.measure('cknna', feat_A, feat_B, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454eace-71a8-4cf1-95c4-2f3ef372b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd878682-aff3-4567-99d2-1d3448ab4228",
   "metadata": {},
   "source": [
    "# 2.1 - Analyse feature alignment and correlation with model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6da4f680-ffd9-42c1-a905-9c44a1aee50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment_score(model, layer_idx=8, batch_size=8, topk=2, use_projection=True, t_start=0., t_end=1., small_bs=16, metrics='cknna', temp=1.):\n",
    "    # change layer idx for extracting features from the \n",
    "    model.encoder_depth = layer_idx\n",
    "    \n",
    "    # extract features\n",
    "    feat_A = []\n",
    "    feat_B = []\n",
    "    for _ in range(batch_size // small_bs):\n",
    "        features = get_batch_features(\n",
    "            model=model,\n",
    "            vae=vae,\n",
    "            encoders=encoders,\n",
    "            encoder_types=encoder_types,\n",
    "            architectures=architectures,\n",
    "            train_dataset=train_dataset,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device,\n",
    "            # batch_size=batch_size,\n",
    "            batch_size=small_bs,\n",
    "            use_projection=use_projection,\n",
    "            t_start=t_start,\n",
    "            t_end=t_end,\n",
    "        )\n",
    "        feat_A.append(features['zs'][0])\n",
    "        feat_B.append(features['zs_tilde'][0])\n",
    "\n",
    "    # compute alignment score\n",
    "    # alignment score\n",
    "    # feat_A = features['zs'][0]#.mean(dim=1) # B, D\n",
    "    # feat_B = features['zs_tilde'][0]#.mean(dim=1) # # B, D\n",
    "    # feat_A = torch.cat(feat_A, dim=0).mean(dim=1)\n",
    "    # feat_B = torch.cat(feat_B, dim=0).mean(dim=1)\n",
    "    # alignment_score = AlignmentMetrics.measure('cknna', feat_A, feat_B, topk=topk)\n",
    "    # alignment_score = AlignmentMetrics.patch2patch_kernel_alignment_score(feat_A, feat_B)\n",
    "    # alignment_score = AlignmentMetrics.sample2sample_kernel_alignment_score(feat_A, feat_B)\n",
    "    # alignment_score = patch2patch_kernel_alignment_score(feat_A, feat_B)\n",
    "\n",
    "    rtn = {}\n",
    "    for metric in metrics.split(\",\"):\n",
    "        metric = metric.strip()\n",
    "        if \"kernel_alignment_score\" in metric:\n",
    "            # Preprocess features for our kernel alignment score dimension requirements\n",
    "            feat_A = features['zs'][0]#.mean(dim=1) # B, D\n",
    "            feat_B = features['zs_tilde'][0]#.mean(dim=1) # # B, D\n",
    "        else:\n",
    "            feat_A = torch.cat(feat_A, dim=0).mean(dim=1)\n",
    "            feat_B = torch.cat(feat_B, dim=0).mean(dim=1)\n",
    "\n",
    "        if \"nn\" in metric:\n",
    "            # NN-based metric require topk argument\n",
    "            kwargs = {\"topk\": topk}\n",
    "        elif 'jsd' in metric:\n",
    "            kwargs = {\"temperature\": temp}\n",
    "        else:\n",
    "            kwargs = {}\n",
    "        alignment_score = AlignmentMetrics.measure(metric, feat_A, feat_B, **kwargs)\n",
    "        rtn[metric] = alignment_score\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6643abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361b1d8-7575-414a-be56-0c1c8f49b4d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# hyperparams\n",
    "batch_size=512\n",
    "topk=10\n",
    "layer_start=1\n",
    "layer_end=28\n",
    "use_projection = False\n",
    "t_start = 0.0\n",
    "t_end = 0.01\n",
    "# metrics = \"cknna\"\n",
    "# metrics = \"sample2sample_kernel_alignment_score\"\n",
    "# metrics = \"patch2patch_kernel_alignment_score\"\n",
    "# metrics = \"sample2sample_kernel_alignment_score_kl_div\"\n",
    "# metrics = \"patch2patch_kernel_alignment_score_kl_div\"\n",
    "metrics = \"patch2patch_kernel_alignment_score_jsd\"\n",
    "# metrics = \",\".join([\"cknna\", \"sample2sample_kernel_alignment_score\", \"patch2patch_kernel_alignment_score\", \"sample2sample_kernel_alignment_score_kl_div\", \"patch2patch_kernel_alignment_score_kl_div\"])\n",
    "\n",
    "alignment_scores = []\n",
    "# Compute alignment scores with tqdm for progress\n",
    "for layer_idx in tqdm(range(layer_start,layer_end), desc=\"Computing Alignment Scores\"):\n",
    "    score = get_alignment_score(model, layer_idx=layer_idx, batch_size=batch_size, topk=topk, use_projection=use_projection, t_start=t_start, t_end=t_end, small_bs=8, metrics=metrics)\n",
    "    alignment_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6e29b-55bb-4d73-b030-50b715756f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ce6c8-600c-4450-b839-5b30dc872805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the alignment scores\n",
    "# metric = \"cknna\"\n",
    "# metric = \"sample2sample_kernel_alignment_score\"\n",
    "# metric = \"patch2patch_kernel_alignment_score\"\n",
    "# metric = \"sample2sample_kernel_alignment_score_kl_div\"\n",
    "metric = \"patch2patch_kernel_alignment_score_kl_div\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(layer_start,layer_end), -np.array([elem[metric] for elem in alignment_scores]), marker='o', linestyle='-', color='b', label='Alignment Score')\n",
    "plt.title(f\"Alignment Scores Across Layers (Batch Size: {batch_size}, TopK: {topk}, tstart: {t_start}, tend: {t_end}, metric: {metric})\", fontsize=14)\n",
    "plt.xlabel(\"Layer Index\", fontsize=12)\n",
    "plt.ylabel(\"Alignment Score\", fontsize=12)\n",
    "plt.ylim(-1, 0)\n",
    "plt.xticks(range(layer_start, layer_end, 1))\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeee314",
   "metadata": {},
   "source": [
    "# 2.2 - The training steps v.s. alignment score curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6027d40f-69d3-47ac-b4e4-6542cce0e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_alignment_across_layers(model, batch_size=256, layer_start=1, layer_end=28, topk=10, use_projection=False, t_start=0., t_end=1., small_bs=16, metrics=\"cknna\", temp=0.1):\n",
    "    alignment_scores = []\n",
    "    metric = [elem.strip() for elem in metrics.split(\",\")]\n",
    "    assert len(metric) == 1, \"Only one metric is supported for this function for now.\"\n",
    "    metric = metric[0]\n",
    "    # Compute alignment scores with tqdm for progress\n",
    "    for layer_idx in tqdm(range(layer_start,layer_end), desc=\"Computing Alignment Scores\"):\n",
    "        score = get_alignment_score(model=model, layer_idx=layer_idx, batch_size=batch_size, topk=topk, use_projection=use_projection, t_start=t_start, t_end=t_end, small_bs=small_bs, metrics=metrics, temp=temp)\n",
    "        alignment_scores.append(score)\n",
    "    alignment_scores = [elem[metric] for elem in alignment_scores]  # <-- the metric\n",
    "    alignment_scores = torch.tensor(alignment_scores)\n",
    "    return torch.max(alignment_scores).item(), torch.argmax(alignment_scores).item() + layer_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e1a5d-be77-41a3-b91b-3726405abcd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 512\n",
    "layer_start = 4\n",
    "layer_end = 20\n",
    "topk = 10\n",
    "use_projection = False\n",
    "t_start = 0.499\n",
    "t_end = 0.501\n",
    "metrics = \"cknna\"\n",
    "\n",
    "max_alignment_score, max_alignment_score_pos = get_max_alignment_across_layers(\n",
    "    model=model,\n",
    "    batch_size=batch_size,\n",
    "    layer_start=layer_start,\n",
    "    layer_end=layer_end,\n",
    "    topk=topk,\n",
    "    use_projection=use_projection,\n",
    "    t_start=t_start,\n",
    "    t_end=t_end,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "print(f\"Max alignment score: {max_alignment_score:.4f} @ layer {max_alignment_score_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9460e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del model\n",
    "del state_dict\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce87d0-2cec-424a-94a1-35ac01b73128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different model checkpoints and see the trend\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@dataclass\n",
    "class repaArgs():\n",
    "    vae = 'ema'\n",
    "    # model = \"SiT-L/2\"\n",
    "    # model = 'SiT-XL/2'\n",
    "    model = \"SiT-B/2\"\n",
    "    num_classes = 1000\n",
    "    encoder_depth = 8\n",
    "    projector_embed_dims = \"768\"\n",
    "    # projector_embed_dims = \"1024\"\n",
    "    fused_attn = True\n",
    "    qk_norm = False\n",
    "    resolution = 256\n",
    "    # ckpt_steps_path = 'exps/sit-l-linear-dinov2-l-enc8-400k/checkpoints'\n",
    "    ckpt_steps_path = 'exps/sit-xl-linear-dinov2-b-enc8-400k-full/checkpoints'\n",
    "    # ckpt_steps_path = \"exps/sit-xl-linear-dinov2-b-enc8-400k/checkpoints\"\n",
    "    # ckpt_steps_path = 'exps/sit-b-linear-dinov2-b-enc8-400k-full/checkpoints'\n",
    "    # ckpt_steps_path = \"exps/sit-b-base-400k-full/checkpoints\"\n",
    "    data_dir = './data/'\n",
    "    batch_size = 512\n",
    "    # layer_start = 1\n",
    "    # layer_end = 28  # SiT-XL/2\n",
    "    # layer_end = 12  # SiT-B/2\n",
    "    layer_start = 8  # curr exp\n",
    "    layer_end = 9  # curr exp\n",
    "    topk = 10\n",
    "    use_projection = False\n",
    "    t_start = 0.499\n",
    "    t_end = 0.501\n",
    "    # small_bs = 32  # SiT-B/2\n",
    "    small_bs = 256\n",
    "    # metrics = \"cknna\"\n",
    "    metrics = \"patch2patch_kernel_alignment_score_jsd\"\n",
    "    temp=0.1\n",
    "    # metrics = \"sample2sample_kernel_alignment_score_jsd\"\n",
    "    # temp=0.2\n",
    "\n",
    "args = repaArgs()\n",
    "device = \"cuda:0\"\n",
    "\n",
    "ckpt_paths = [os.path.join(args.ckpt_steps_path, elem) for elem in sorted(os.listdir(args.ckpt_steps_path)) if elem.endswith('.pt')]\n",
    "steps = [int(elem.split('/')[-1].split('.')[0]) for elem in ckpt_paths]\n",
    "max_alignment_scores = []\n",
    "\n",
    "for step, ckpt_path in tqdm(zip(steps, ckpt_paths), total=len(steps)):\n",
    "    block_kwargs = {\"fused_attn\": args.fused_attn, \"qk_norm\": args.qk_norm}\n",
    "    latent_size = args.resolution // 8\n",
    "    model = SiT_models[args.model](\n",
    "        input_size=latent_size,\n",
    "        num_classes=args.num_classes,\n",
    "        use_cfg = True,\n",
    "        z_dims = [int(z_dim) for z_dim in args.projector_embed_dims.split(',')],\n",
    "        encoder_depth=args.encoder_depth,\n",
    "        **block_kwargs,\n",
    "    ).to(device)\n",
    "\n",
    "    state_dict = torch.load(ckpt_path, map_location=device)#['ema']\n",
    "    if \"model\" in state_dict:\n",
    "        state_dict = state_dict[\"model\"]\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()  # important!\n",
    "\n",
    "    max_alignment_score, max_alignment_score_pos = get_max_alignment_across_layers(\n",
    "        model=model,\n",
    "        batch_size=args.batch_size,\n",
    "        layer_start=args.layer_start,\n",
    "        layer_end=args.layer_end,\n",
    "        topk=args.topk,\n",
    "        use_projection=args.use_projection,\n",
    "        t_start=args.t_start,\n",
    "        t_end=args.t_end,\n",
    "        small_bs=args.small_bs,\n",
    "        metrics=args.metrics,\n",
    "    )\n",
    "    max_alignment_scores.append(max_alignment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed27951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, max_alignment_scores, marker='o', linestyle='-', color='b', label='Alignment Score')\n",
    "plt.title(f\"Layer Max Alignment Scores Across Steps (Batch Size: {args.batch_size}, TopK: {args.topk}, tstart: {args.t_start}, tend: {args.t_end})\", fontsize=14)\n",
    "plt.xlabel(\"Training steps\", fontsize=12)\n",
    "plt.ylabel(\"Alignment Score (Layer Max)\", fontsize=12)\n",
    "# plt.ylim(0, 1)\n",
    "plt.xticks(steps)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, max_alignment_scores, marker='o', linestyle='-', color='b', label='Alignment Score')\n",
    "plt.title(f\"Layer Max Alignment Scores Across Steps (Batch Size: {args.batch_size}, TopK: {args.topk}, tstart: {args.t_start}, tend: {args.t_end})\", fontsize=14)\n",
    "plt.xlabel(\"Training steps\", fontsize=12)\n",
    "plt.ylabel(\"Alignment Score (Layer Max)\", fontsize=12)\n",
    "# plt.ylim(0, 1)\n",
    "plt.xticks(steps)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f85a6-d752-4a03-8335-860d46dd92e9",
   "metadata": {},
   "source": [
    "# 3 - Design kernel alignment metric and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54a959-9db0-4b9d-9c15-89cacd2e2b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f0c6a-0f8a-4cac-9a36-77e32c6b9992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69695dd-325f-48ef-92a2-3d0c15aa9129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff2eb5-2733-4564-befc-de096dd01dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d320a06c-ff02-4fb4-8b0a-dfdbc62478d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0fbbe-1953-4fbb-8606-9d4de0ad7411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f61517-dbb0-4ace-8926-8748655ca2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
