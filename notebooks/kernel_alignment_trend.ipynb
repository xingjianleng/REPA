{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from utils import load_encoders\n",
    "from models.sit import SiT_models\n",
    "from diffusers import AutoencoderKL\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import PIL\n",
    "import gc\n",
    "import io\n",
    "from torchvision.transforms import Normalize\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pyspng = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the data\n",
    "with open(\"data/images_h5.json\", \"r\") as f:\n",
    "    images_h5_cfg = json.load(f)\n",
    "with open(\"data/vae-sd_h5.json\", \"r\") as f:\n",
    "    vae_h5_cfg = json.load(f)\n",
    "\n",
    "def load_h5_file(hf, path):\n",
    "    # Helper function to load files from h5 file\n",
    "    if path.endswith('.png'):\n",
    "        if pyspng is not None:\n",
    "            rtn = pyspng.load(io.BytesIO(np.array(hf[path])))\n",
    "        else:\n",
    "            rtn = np.array(PIL.Image.open(io.BytesIO(np.array(hf[path]))))\n",
    "        rtn = rtn.reshape(*rtn.shape[:2], -1).transpose(2, 0, 1)\n",
    "    elif path.endswith('.json'):\n",
    "        rtn = json.loads(np.array(hf[path]).tobytes().decode('utf-8'))\n",
    "    elif path.endswith('.npy'):\n",
    "        rtn= np.array(hf[path])\n",
    "    else:\n",
    "        raise ValueError('Unknown file type: {}'.format(path))\n",
    "    return rtn\n",
    "\n",
    "def preprocess_raw_image(x, enc_type):\n",
    "    x = x / 255.\n",
    "    x = Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)(x)\n",
    "    x = torch.nn.functional.interpolate(x, 224, mode='bicubic')\n",
    "    x = torch.nn.functional.interpolate(x, 224, mode='bicubic')\n",
    "    return x\n",
    "\n",
    "N = 1024\n",
    "BS = 8\n",
    "chosen_files = random.Random(42).sample(images_h5_cfg, N)\n",
    "chosen_vaes = [elem.replace(\"img\", \"img-mean-std-\").replace(\".png\", \".npy\") for elem in chosen_files]\n",
    "\n",
    "image_h5 = h5py.File(\"data/images.h5\", \"r\")\n",
    "vae_h5 = h5py.File(\"data/vae-sd.h5\", \"r\")\n",
    "\n",
    "### Labels...\n",
    "fname = 'dataset.json'\n",
    "labels = load_h5_file(vae_h5, fname)['labels']\n",
    "labels = dict(labels)\n",
    "labels = [labels[fname.replace('\\\\', '/')] for fname in chosen_vaes]\n",
    "labels = np.array(labels)\n",
    "labels = labels.astype({1: np.int64, 2: np.float32}[labels.ndim])\n",
    "\n",
    "images = preprocess_raw_image(torch.stack([torch.from_numpy(load_h5_file(image_h5, elem)) for elem in chosen_files]), \"dinov2-vit-b\")\n",
    "vaes = torch.stack([torch.from_numpy(load_h5_file(vae_h5, elem)) for elem in chosen_vaes])\n",
    "labels = torch.from_numpy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load other models\n",
    "encoders, encoder_types, architectures = load_encoders(\"dinov2-vit-b\", \"cuda:0\")\n",
    "encoder, encoder_type, architecture = encoders[0], encoder_types[0], architectures[0]\n",
    "# vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(\"cuda:0\")\n",
    "\n",
    "### Model HPs\n",
    "model_name = \"SiT-B/2\"\n",
    "resolution = 256\n",
    "num_classes = 1000\n",
    "assert resolution % 8 == 0\n",
    "latent_size = resolution // 8\n",
    "z_dims = [encoder.embed_dim for encoder in encoders]\n",
    "encoder_depth = 8\n",
    "block_kwargs = {\"fused_attn\": False, \"qk_norm\": False}\n",
    "\n",
    "\n",
    "def get_model(ckpt_path):\n",
    "    model = SiT_models[model_name](\n",
    "        latent_size=latent_size,\n",
    "        num_classes=num_classes,\n",
    "        use_cfg=True,\n",
    "        z_dims=z_dims,\n",
    "        encoder_depth=encoder_depth,\n",
    "        **block_kwargs,\n",
    "    ).to(\"cuda:0\")\n",
    "    state_dict = torch.load(ckpt_path, map_location=\"cuda:0\")\n",
    "    model.load_state_dict(state_dict[\"model\"])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def interpolant(t):\n",
    "    alpha_t = 1 - t\n",
    "    sigma_t = t\n",
    "    d_alpha_t = -1\n",
    "    d_sigma_t = 1\n",
    "    return alpha_t, sigma_t, d_alpha_t, d_sigma_t\n",
    "\n",
    "def mean_flat(x):\n",
    "    return torch.mean(x, dim=list(range(1, len(x.size()))))\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_posterior(moments, latents_scale=1., latents_bias=0.):\n",
    "    mean, std = torch.chunk(moments, 2, dim=1)\n",
    "    z = mean + std * torch.randn_like(mean)\n",
    "    z = (z * latents_scale + latents_bias) \n",
    "    return z\n",
    "\n",
    "latents_scale = torch.tensor(\n",
    "    [0.18215, 0.18215, 0.18215, 0.18215]\n",
    ").view(1, 4, 1, 1).to(\"cuda:0\")\n",
    "latents_bias = torch.tensor(\n",
    "    [0., 0., 0., 0.]\n",
    ").view(1, 4, 1, 1).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_paths = [\"exps/sit-b-base-400k/checkpoints\", \"exps/sit-b-linear-dinov2-b-enc8-400k/checkpoints\"]\n",
    "\n",
    "zs = []\n",
    "for i in tqdm(range(0, N, BS)):\n",
    "    images_batch = preprocess_raw_image(images[i:i+BS].to(\"cuda:0\"), encoder_type)\n",
    "    with torch.no_grad():\n",
    "        z = encoder.forward_features(images_batch)['x_norm_patchtokens']\n",
    "        zs.append(z)\n",
    "zs = torch.cat(zs, dim=0).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_stats = {}\n",
    "\n",
    "for ckpt_path in ckpt_paths:\n",
    "    stats_neg_cos_sim = {}\n",
    "    stats_frobenius_l2_norm = {}\n",
    "\n",
    "    for elem in sorted(os.listdir(ckpt_path)):\n",
    "        model = get_model(os.path.join(ckpt_path, elem))\n",
    "\n",
    "        # Call gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        zs_tilde = []\n",
    "        for i in tqdm(range(0, N, BS)):\n",
    "            vaes_batch = vaes[i:i+BS].to(\"cuda:0\")\n",
    "            labels_batch = labels[i:i+BS].to(\"cuda:0\")\n",
    "            x = sample_posterior(vaes_batch, latents_scale=latents_scale, latents_bias=latents_bias)\n",
    "            model_kwargs = dict(y = labels_batch)\n",
    "            time_input = torch.rand((x.shape[0], 1, 1, 1), device=\"cuda:0\", dtype=x.dtype)\n",
    "\n",
    "            noises = torch.randn_like(x)\n",
    "            alpha_t, sigma_t, d_alpha_t, d_sigma_t = interpolant(time_input)  # linear\n",
    "\n",
    "            model_input = alpha_t * x + sigma_t * noises\n",
    "            model_target = d_alpha_t * x + d_sigma_t * noises\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_output, z_tilde = model(model_input, time_input.flatten(), **model_kwargs)\n",
    "                zs_tilde.append(z_tilde[0])\n",
    "        zs_tilde = torch.cat(zs_tilde, dim=0).cpu()\n",
    "        \n",
    "        # Compare current zs_tilde with zs\n",
    "        # Part 1: cos-sim\n",
    "        proj_loss = 0.\n",
    "        for j, (z_j, z_tilde_j) in enumerate(zip(zs, zs_tilde)):\n",
    "            z_tilde_j = torch.nn.functional.normalize(z_tilde_j, dim=-1) \n",
    "            z_j = torch.nn.functional.normalize(z_j, dim=-1) \n",
    "            proj_loss += mean_flat(-(z_j * z_tilde_j).sum(dim=-1))\n",
    "        proj_loss /= N\n",
    "        stats_neg_cos_sim[int(elem[:-3])] = proj_loss.item()\n",
    "\n",
    "        # Part 2: L2 matrix loss\n",
    "        proj_loss = 0.\n",
    "        a_mat = F.normalize(zs @ zs.transpose(1, 2), dim=-1)\n",
    "        a_tilde_mat = F.normalize(zs_tilde @ zs_tilde.transpose(1, 2), dim=-1)\n",
    "        # Compute the element-wise loss\n",
    "        proj_loss += torch.sqrt(F.mse_loss(a_mat, a_tilde_mat, reduction='mean'))\n",
    "        stats_frobenius_l2_norm[int(elem[:-3])] = proj_loss.item()\n",
    "    \n",
    "    ckpt_stats[ckpt_path] = {\"neg_cos_sim\": stats_neg_cos_sim, \"kernel_align_patch_l2\": stats_frobenius_l2_norm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg-cos-sim\n",
    "plt.figure(figsize=(8, 6))\n",
    "for k, v in ckpt_stats.items():\n",
    "    plt.plot(list(v[\"neg_cos_sim\"].keys()), list(v[\"neg_cos_sim\"].values()), label=k, marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Neg Cosine Similarity\")\n",
    "plt.title(\"Neg Cosine Similarity v.s. Steps\")\n",
    "plt.show()\n",
    "\n",
    "# kernel align patch l2\n",
    "plt.figure(figsize=(8, 6))\n",
    "for k, v in ckpt_stats.items():\n",
    "    plt.plot(list(v[\"kernel_align_patch_l2\"].keys()), list(v[\"kernel_align_patch_l2\"].values()), label=k, marker='o')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Kernel Align Patch L2\")\n",
    "plt.title(\"Kernel Align Patch L2 v.s. Steps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
