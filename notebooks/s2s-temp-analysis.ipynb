{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Quick import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch3/zha439/REPA\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from models.sit import SiT_models\n",
    "from metrics import AlignmentMetrics\n",
    "from diffusers import AutoencoderKL\n",
    "from dataset import CustomDataset, CustomZipDataset, CustomH5Dataset\n",
    "from utils import load_encoders\n",
    "from loss import SILoss\n",
    "import einops\n",
    "from train import sample_posterior, preprocess_raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_alongside_batch(img_list, resize_dims=(512,512)):\n",
    "    if isinstance(resize_dims, int):\n",
    "        resize_dims = (resize_dims,resize_dims)\n",
    "    res = np.concatenate([np.array(img.resize(resize_dims)) for img in img_list], axis=1)\n",
    "    return Image.fromarray(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the SiT models and get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class repaArgs():\n",
    "    vae = 'ema'\n",
    "    enc_type = \"dinov2\"\n",
    "    model =  'SiT-XL/2'\n",
    "    num_classes = 1000\n",
    "    encoder_depth_repa = [8]\n",
    "    encoder_depth_ka = [8]\n",
    "    projector_embed_dims = \"768\"\n",
    "    fused_attn = True\n",
    "    qk_norm = False\n",
    "    resolution = 256\n",
    "    ckpt = 'exps/sit-xl-linear-dinov2-b-enc8-sample2sample-jsd-coeff0.2-50k/checkpoints/0020000.pt'\n",
    "    # ckpt = 'exps/sit-xl-linear-dinov2-b-enc8-sample2sample-v2-jsd-0.2-0.2-only-coeff0.2-50k/checkpoints/0020000.pt'\n",
    "    data_dir = './data/'\n",
    "    enc_type: str = \"dinov2-vit-b\"\n",
    "    path_type = \"linear\"\n",
    "    batch_size = 256\n",
    "    batch_size_per_gpu = 64\n",
    "    weighting = \"uniform\"\n",
    "    prediction = \"v\"\n",
    "    s2s_src_temp = 0.2\n",
    "    s2s_tgt_temp = 0.2\n",
    "\n",
    "args = repaArgs()\n",
    "device = \"cuda:0\"\n",
    "\n",
    "block_kwargs = {\"fused_attn\": args.fused_attn, \"qk_norm\": args.qk_norm}\n",
    "latent_size = args.resolution // 8\n",
    "model = SiT_models[args.model](\n",
    "    input_size=latent_size,\n",
    "    num_classes=args.num_classes,\n",
    "    use_cfg = True,\n",
    "    z_dims = [int(z_dim) for z_dim in args.projector_embed_dims.split(',')],\n",
    "    encoder_depth_repa=args.encoder_depth_repa,\n",
    "    encoder_depth_ka=args.encoder_depth_ka,\n",
    "    **block_kwargs,\n",
    ").to(device)\n",
    "\n",
    "# Auto-download a pre-trained model or load a custom SiT checkpoint from train.py:\n",
    "ckpt_path = args.ckpt\n",
    "state_dict = torch.load(ckpt_path, map_location=f'{device}')#['ema']\n",
    "if \"model\" in state_dict:\n",
    "    state_dict = state_dict[\"model\"]\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  # important!\n",
    "vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{args.vae}\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zha439/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/zha439/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/zha439/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/zha439/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    "def load_train_dataset(args):\n",
    "    if (os.path.exists(os.path.join(args.data_dir, \"images\")) and\n",
    "        os.path.exists(os.path.join(args.data_dir, \"vae-sd\"))):\n",
    "        train_dataset = CustomDataset(args.data_dir)\n",
    "    elif (os.path.exists(os.path.join(args.data_dir, \"images.h5\")) and\n",
    "          os.path.exists(os.path.join(args.data_dir, \"vae-sd.h5\")) and\n",
    "          os.path.exists(os.path.join(args.data_dir, \"images_h5.json\")) and\n",
    "          os.path.exists(os.path.join(args.data_dir, \"vae-sd_h5.json\"))):\n",
    "            train_dataset = CustomH5Dataset(args.data_dir)\n",
    "    elif (os.path.exists(os.path.join(args.data_dir, \"images.zip\")) and\n",
    "          os.path.exists(os.path.join(args.data_dir, \"vae-sd.zip\"))):\n",
    "        train_dataset = CustomZipDataset(args.data_dir)\n",
    "    else:\n",
    "        raise ValueError(\"Dataset not found.\")\n",
    "    return train_dataset\n",
    "\n",
    "def load_perception_encoders(args):\n",
    "    if args.enc_type != 'None':\n",
    "        encoders, encoder_types, architectures = load_encoders(args.enc_type, device)\n",
    "    else:\n",
    "        encoders, encoder_types, architectures = [None], [None], [None]\n",
    "    return encoders, encoder_types, architectures\n",
    "\n",
    "def get_loss_fn(args):\n",
    "    # create loss function\n",
    "    loss_fn = SILoss(\n",
    "        prediction=args.prediction,\n",
    "        path_type=args.path_type, \n",
    "        encoders=encoders,\n",
    "        accelerator=None, # since we only need the interpolation fn\n",
    "        latents_scale=None,# since we only need the interpolation fn\n",
    "        latents_bias=None,# since we only need the interpolation fn\n",
    "        weighting=args.weighting\n",
    "    )\n",
    "    return loss_fn\n",
    "\n",
    "train_dataset = load_train_dataset(args)  # Uses args.data_dir\n",
    "encoders, encoder_types, architectures = load_perception_encoders(args)\n",
    "loss_fn = get_loss_fn(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_features(\n",
    "    model,\n",
    "    vae,\n",
    "    encoders, \n",
    "    encoder_types, \n",
    "    architectures, \n",
    "    train_dataset, \n",
    "    loss_fn,\n",
    "    device='cuda', \n",
    "    batch_size=8,\n",
    "    ka_use_projection=True, \n",
    "    t_start=0.,\n",
    "    t_end=1.,\n",
    "):\n",
    "    # Setup scale/bias for latents\n",
    "    latents_scale = torch.tensor([0.18215]*4).view(1,4,1,1).to(device)\n",
    "    latents_bias = torch.tensor([0.0]*4).view(1,4,1,1).to(device)\n",
    "\n",
    "    # Get one batch\n",
    "    temp_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    raw_image, x, y = next(iter(temp_loader))\n",
    "    raw_image = raw_image.to(device)\n",
    "    x = x.squeeze(dim=1).to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # If needed, handle cfg_prob / legacy label dropping here. For simplicity:\n",
    "    labels = y\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Convert VAE latents to model latents\n",
    "        x = sample_posterior(x, latents_scale=latents_scale, latents_bias=latents_bias)\n",
    "\n",
    "        # Extract encoder features\n",
    "        zs = []\n",
    "        for encoder, encoder_type, arch in zip(encoders, encoder_types, architectures):\n",
    "            raw_image_ = preprocess_raw_image(raw_image, encoder_type)\n",
    "            z = encoder.forward_features(raw_image_)\n",
    "            if 'mocov3' in encoder_type:\n",
    "                z = z[:, 1:]\n",
    "            if 'dinov2' in encoder_type:\n",
    "                z = z['x_norm_patchtokens']\n",
    "            zs.append(z)\n",
    "\n",
    "        # Sample a random time step (uniform weighting)\n",
    "        time_input = torch.rand((x.shape[0], 1, 1, 1), device=device, dtype=x.dtype)\n",
    "        # limit to the given range\n",
    "        time_input = time_input * (t_end - t_start) + t_start\n",
    "        \n",
    "        alpha_t, sigma_t, d_alpha_t, d_sigma_t = loss_fn.interpolant(time_input)\n",
    "        noises = torch.randn_like(x)\n",
    "        model_input = alpha_t * x + sigma_t * noises\n",
    "        model_target = d_alpha_t * x + d_sigma_t * noises\n",
    "\n",
    "        # model output is for `v` prediction, zs_tilde is for REPA loss computation, fs_tilde is for KA loss computation...\n",
    "        model_output, zs_tilde,  fs_tilde, _, _ = model(model_input, time_input.flatten(), y=labels, ka_use_projection=ka_use_projection, return_all_layers=False)\n",
    "\n",
    "    return {\n",
    "        \"raw_image\": raw_image,\n",
    "        \"model_input\": model_input,\n",
    "        \"zs\": zs,\n",
    "        \"zs_tilde\": zs_tilde,\n",
    "        \"fs_tilde\": fs_tilde,\n",
    "        \"model_output\": model_output,\n",
    "        \"time_input\": time_input,\n",
    "        \"model_target\": model_target,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_stack(tensor_list):\n",
    "    if isinstance(tensor_list, torch.Tensor):\n",
    "        return tensor_list\n",
    "    else:\n",
    "        stacked_tensors = [recursive_stack(item) for item in tensor_list]\n",
    "        return torch.stack(stacked_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 64, 256, 768]) torch.Size([4, 1, 1, 64, 256, 768]) torch.Size([4, 1, 1, 64, 256, 768])\n",
      "torch.Size([1, 256, 256, 768]) torch.Size([1, 1, 256, 256, 768]) torch.Size([1, 1, 256, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# A small batch size, similar to the 64x64 case. Do inference\n",
    "\n",
    "zs = []\n",
    "zs_tilde = []\n",
    "fs_tilde = []\n",
    "\n",
    "\n",
    "for _ in range(args.batch_size // args.batch_size_per_gpu):\n",
    "    batch = get_batch_features(\n",
    "        model,\n",
    "        vae,\n",
    "        encoders, \n",
    "        encoder_types, \n",
    "        architectures, \n",
    "        train_dataset, \n",
    "        loss_fn,\n",
    "        device=device, \n",
    "        batch_size=args.batch_size_per_gpu,\n",
    "        ka_use_projection=True, \n",
    "        t_start=0.,\n",
    "        t_end=1.,\n",
    "    )\n",
    "    zs.append(batch[\"zs\"])\n",
    "    zs_tilde.append(batch[\"zs_tilde\"])\n",
    "    fs_tilde.append(batch[\"fs_tilde\"])\n",
    "\n",
    "zs = recursive_stack(zs)\n",
    "zs_tilde = recursive_stack(zs_tilde)\n",
    "fs_tilde = recursive_stack(fs_tilde)\n",
    "\n",
    "zs_gather = einops.rearrange(zs, 'g p b n d -> p (g b) n d')\n",
    "zs_tilde_gather = einops.rearrange(zs_tilde, 'g l p b n d -> l p (g b) n d')\n",
    "fs_tilde_gather = einops.rearrange(fs_tilde, 'g l p b n d -> l p (g b) n d')\n",
    "\n",
    "print(zs.shape, zs_tilde.shape, fs_tilde.shape)\n",
    "print(zs_gather.shape, zs_tilde_gather.shape, fs_tilde_gather.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample2sample_kernel_alignment_score_jsd(feats_A, feats_B, feats_A_=None, feats_B_=None, src_temp=1.0, tgt_temp=1.0, detach_grad=False):\n",
    "    \"\"\"\n",
    "    Compute a sample-to-sample kernel alignment score using Jensen-Shannon Divergence.\n",
    "    1. Average feats_A and feats_B across the N dimension to get (B, D) and (B, E).\n",
    "    2. Normalize features along the last dimension.\n",
    "    3. Compute sample-to-sample similarity matrices for A and B (B, B).\n",
    "    4. Convert these matrices to probability distributions (softmax).\n",
    "    5. Compute the JSD between these distributions.\n",
    "    6. Convert JSD to an alignment score = 1 - JSD_mean, return as a scalar.\n",
    "\n",
    "    Args:\n",
    "        feats_A: (B, N, D)\n",
    "        feats_B: (B, N, E)\n",
    "        feats_A_: (B', N, D) (if provided)\n",
    "        feats_B_: (B', N, E) (if provided)\n",
    "        src_temp: float, temperature for softmax applied to feats_A\n",
    "        tgt_temp: float, temperature for softmax applied to feats_B\n",
    "        detach_grad: bool, detach the gradient of the part of the feature matrix\n",
    "\n",
    "    Returns:\n",
    "        alignment_score: a scalar float value.\n",
    "    \"\"\"\n",
    "    # Average features across the N dimension: (B, N, D) -> (B, D)\n",
    "    feats_A = feats_A.mean(dim=1)\n",
    "    feats_B = feats_B.mean(dim=1)\n",
    "\n",
    "    # Normalize the features\n",
    "    feats_A = F.normalize(feats_A, dim=-1)\n",
    "    feats_B = F.normalize(feats_B, dim=-1)\n",
    "\n",
    "    if feats_A_ is None:\n",
    "        if detach_grad:\n",
    "            feats_A_ = feats_A.clone().detach()\n",
    "        else:\n",
    "            feats_A_ = feats_A\n",
    "    else:\n",
    "        feats_A_ = feats_A_.mean(dim=1)\n",
    "        feats_A_ = F.normalize(feats_A_, dim=-1)\n",
    "\n",
    "    if feats_B_ is None:\n",
    "        if detach_grad:\n",
    "            feats_B_ = feats_B.clone().detach()\n",
    "        else:\n",
    "            feats_B_ = feats_B\n",
    "    else:\n",
    "        feats_B_ = feats_B_.mean(dim=1)\n",
    "        feats_B_ = F.normalize(feats_B_, dim=-1)\n",
    "\n",
    "    # Compute similarity matrices (B, B)\n",
    "    kernel_matrix_A = feats_A @ feats_A_.transpose(0, 1)\n",
    "    kernel_matrix_B = feats_B @ feats_B_.transpose(0, 1)\n",
    "    # print(kernel_matrix_A.shape, kernel_matrix_B.shape)\n",
    "\n",
    "    # Convert similarities to probability distributions\n",
    "    P = F.softmax(kernel_matrix_A / src_temp, dim=-1)  # (B, B)\n",
    "    Q = F.softmax(kernel_matrix_B / tgt_temp, dim=-1)  # (B, B)\n",
    "    M = 0.5 * (P + Q)  # Mixture distribution\n",
    "\n",
    "    eps = 1e-10\n",
    "    P_clamped = P.clamp(min=eps)\n",
    "    Q_clamped = Q.clamp(min=eps)\n",
    "    M_clamped = M.clamp(min=eps)\n",
    "\n",
    "    # Compute KL divergences using F.kl_div\n",
    "    # KL(P||M) = sum(P * log(P/M)) over dim=-1\n",
    "    # Using F.kl_div: input=logM, target=P\n",
    "    # logM = M_clamped.log()\n",
    "\n",
    "    # Compute KL(P||M) and KL(Q||M) row-wise\n",
    "    # KL(P||M) = sum_over_j P_j * log(P_j/M_j)\n",
    "    KL_PM = (P_clamped * (P_clamped.log() - M_clamped.log())).sum(dim=-1)  # (B)\n",
    "    KL_QM = (Q_clamped * (Q_clamped.log() - M_clamped.log())).sum(dim=-1)  # (B)\n",
    "\n",
    "    # # equivalent using F.kl_div\n",
    "    # KL_PM = F.kl_div(logM, P_clamped, reduction='none', log_target=False).sum(dim=-1)  # (B,)\n",
    "    # KL_QM = F.kl_div(logM, Q_clamped, reduction='none', log_target=False).sum(dim=-1)  # (B,)\n",
    "\n",
    "    # JSD in base 2\n",
    "    log2 = torch.log(torch.tensor(2.0))\n",
    "    JSD = 0.5 * (KL_PM + KL_QM) / log2  # (B,)\n",
    "\n",
    "    # Mean JSD across batch\n",
    "    JSD_mean = JSD.mean()\n",
    "\n",
    "    # Alignment score: 1 - JSD\n",
    "    alignment_score = 1.0 - JSD_mean\n",
    "    return alignment_score, kernel_matrix_A, kernel_matrix_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9568654298782349\n"
     ]
    }
   ],
   "source": [
    "# Simulate 64x64 kernel matrix\n",
    "gpu_id = 2\n",
    "\n",
    "kernel_alignment_loss = 0.0\n",
    "kernel_matrices_A = []\n",
    "kernel_matrices_B = []\n",
    "\n",
    "for fs_tilde_layer in fs_tilde[gpu_id]:\n",
    "    for i, (z, f_tilde_layer) in enumerate(zip(zs[gpu_id], fs_tilde_layer)):\n",
    "        curr_kernel_alignment_loss, kernel_matrix_A, kernel_matrix_B = sample2sample_kernel_alignment_score_jsd(\n",
    "            z,\n",
    "            f_tilde_layer,\n",
    "            src_temp=args.s2s_src_temp,\n",
    "            tgt_temp=args.s2s_tgt_temp,\n",
    "            detach_grad=False,\n",
    "        )\n",
    "        kernel_alignment_loss -= curr_kernel_alignment_loss\n",
    "        kernel_matrices_A.append(kernel_matrix_A)\n",
    "        kernel_matrices_B.append(kernel_matrix_B)\n",
    "\n",
    "print(kernel_alignment_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m kernel_matrices_A \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m kernel_matrices_B \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fs_tilde, fs_tilde_g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mfs_tilde\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[43m]\u001b[49m, fs_tilde_gather):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (z, z_g, f_tilde, f_tilde_g) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(zs, zs_gather, fs_tilde, fs_tilde_g)):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "# Simulate 64x256 kernel matrix\n",
    "gpu_id = 2\n",
    "\n",
    "kernel_alignment_loss = 0.0\n",
    "kernel_matrices_A = []\n",
    "kernel_matrices_B = []\n",
    "\n",
    "for fs_tilde_layer, fs_tilde_g in zip(fs_tilde[gpu_id], fs_tilde_gather):\n",
    "    for i, (z, z_g, f_tilde, f_tilde_g) in enumerate(zip(zs, zs_gather, fs_tilde_layer, fs_tilde_g)):\n",
    "        print(z.shape)\n",
    "        print(z_g.shape)\n",
    "        print(f_tilde.shape)\n",
    "        print(f_tilde_g.shape)\n",
    "        curr_kernel_alignment_loss, kernel_matrices_A, kernel_matrices_B = sample2sample_kernel_alignment_score_jsd(\n",
    "            feats_A=z,  # [B, N, D]\n",
    "            feats_B=f_tilde, # [B, N, D]\n",
    "            feats_A_=z_g,       # [B x n_gpus, N, D]\n",
    "            feats_B_=f_tilde_g,  # [B x n_gpus, N, D]\n",
    "            src_temp=args.s2s_src_temp,\n",
    "            tgt_temp=args.s2s_tgt_temp,\n",
    "            detach_grad=False,\n",
    "        )\n",
    "        kernel_alignment_loss -= curr_kernel_alignment_loss\n",
    "        kernel_matrices_A.append(kernel_matrix_A)\n",
    "        kernel_matrices_B.append(kernel_matrix_B)\n",
    "\n",
    "print(kernel_alignment_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
