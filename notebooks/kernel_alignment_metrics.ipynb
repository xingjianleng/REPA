{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from utils import load_encoders\n",
    "from models.sit import SiT_models\n",
    "from diffusers import AutoencoderKL\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import PIL\n",
    "import io\n",
    "from torchvision.transforms import Normalize\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pyspng = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_DEFAULT_MEAN = (0.48145466, 0.4578275, 0.40821073)\n",
    "CLIP_DEFAULT_STD = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "### Load encoders\n",
    "encoders, encoder_types, architectures = load_encoders(\"dinov2-vit-b\", \"cuda:0\")\n",
    "\n",
    "### Model HPs\n",
    "mode = \"mse\"\n",
    "model = \"SiT-B/2\"\n",
    "# ckpt_path = \"ckpts/sit-b-base-400k-last.pt\"  # <-- Change the checkpoint file here\n",
    "ckpt_path = \"ckpts/sit-b-linear-dinov2-b-enc8-400k-last.pt\"\n",
    "resolution = 256\n",
    "num_classes = 1000\n",
    "assert resolution % 8 == 0\n",
    "latent_size = resolution // 8\n",
    "z_dims = [encoder.embed_dim for encoder in encoders]\n",
    "encoder_depth = 8\n",
    "block_kwargs = {\"fused_attn\": False, \"qk_norm\": False}\n",
    "\n",
    "model = SiT_models[model](\n",
    "    latent_size=latent_size,\n",
    "    num_classes=num_classes,\n",
    "    use_cfg=True,\n",
    "    z_dims=z_dims,\n",
    "    encoder_depth=encoder_depth,\n",
    "    **block_kwargs,\n",
    ").to(\"cuda:0\")\n",
    "state_dict = torch.load(ckpt_path, map_location=\"cuda:0\")\n",
    "\n",
    "\n",
    "### Load weights\n",
    "if mode == \"mse\":\n",
    "    model.load_state_dict(state_dict[\"model\"])\n",
    "elif mode == \"ema\":\n",
    "    model.load_state_dict(state_dict[\"ema\"])\n",
    "model.eval()\n",
    "\n",
    "### Load VAE\n",
    "vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{mode}\").to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the data\n",
    "with open(\"data/images_h5.json\", \"r\") as f:\n",
    "    images_h5_cfg = json.load(f)\n",
    "with open(\"data/vae-sd_h5.json\", \"r\") as f:\n",
    "    vae_h5_cfg = json.load(f)\n",
    "\n",
    "def load_h5_file(hf, path):\n",
    "    # Helper function to load files from h5 file\n",
    "    if path.endswith('.png'):\n",
    "        if pyspng is not None:\n",
    "            rtn = pyspng.load(io.BytesIO(np.array(hf[path])))\n",
    "        else:\n",
    "            rtn = np.array(PIL.Image.open(io.BytesIO(np.array(hf[path]))))\n",
    "        rtn = rtn.reshape(*rtn.shape[:2], -1).transpose(2, 0, 1)\n",
    "    elif path.endswith('.json'):\n",
    "        rtn = json.loads(np.array(hf[path]).tobytes().decode('utf-8'))\n",
    "    elif path.endswith('.npy'):\n",
    "        rtn= np.array(hf[path])\n",
    "    else:\n",
    "        raise ValueError('Unknown file type: {}'.format(path))\n",
    "    return rtn\n",
    "\n",
    "def preprocess_raw_image(x, enc_type):\n",
    "    if 'clip' in enc_type:\n",
    "        x = x / 255.\n",
    "        x = torch.nn.functional.interpolate(x, 224, mode='bicubic')\n",
    "        x = Normalize(CLIP_DEFAULT_MEAN, CLIP_DEFAULT_STD)(x)\n",
    "    elif 'mocov3' in enc_type or 'mae' in enc_type:\n",
    "        x = x / 255.\n",
    "        x = Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)(x)\n",
    "    elif 'dinov2' in enc_type:\n",
    "        x = x / 255.\n",
    "        x = Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)(x)\n",
    "        x = torch.nn.functional.interpolate(x, 224, mode='bicubic')\n",
    "    elif 'dinov1' in enc_type:\n",
    "        x = x / 255.\n",
    "        x = Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)(x)\n",
    "    elif 'jepa' in enc_type:\n",
    "        x = x / 255.\n",
    "        x = Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)(x)\n",
    "        x = torch.nn.functional.interpolate(x, 224, mode='bicubic')\n",
    "\n",
    "    return x\n",
    "\n",
    "N = 4\n",
    "chosen_files = random.Random(42).sample(images_h5_cfg, N)\n",
    "chosen_vaes = [elem.replace(\"img\", \"img-mean-std-\").replace(\".png\", \".npy\") for elem in chosen_files]\n",
    "# print(chosen_files)\n",
    "# print(chosen_vaes)\n",
    "\n",
    "image_h5 = h5py.File(\"data/images.h5\", \"r\")\n",
    "vae_h5 = h5py.File(\"data/vae-sd.h5\", \"r\")\n",
    "\n",
    "### Labels...\n",
    "fname = 'dataset.json'\n",
    "labels = load_h5_file(vae_h5, fname)['labels']\n",
    "labels = dict(labels)\n",
    "labels = [labels[fname.replace('\\\\', '/')] for fname in chosen_vaes]\n",
    "labels = np.array(labels)\n",
    "labels = labels.astype({1: np.int64, 2: np.float32}[labels.ndim])\n",
    "\n",
    "images = preprocess_raw_image(torch.stack([torch.from_numpy(load_h5_file(image_h5, elem)) for elem in chosen_files]), \"dinov2-vit-b\").to(\"cuda:0\")\n",
    "vaes = torch.stack([torch.from_numpy(load_h5_file(vae_h5, elem)) for elem in chosen_vaes]).to(\"cuda:0\")\n",
    "labels = torch.from_numpy(labels).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot three images, 1. raw image, 2. reconstructed ground-truth latent\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_posterior(moments, latents_scale=1., latents_bias=0.):\n",
    "    device = moments.device\n",
    "    \n",
    "    mean, std = torch.chunk(moments, 2, dim=1)\n",
    "    z = mean + std * torch.randn_like(mean)\n",
    "    z = (z * latents_scale + latents_bias) \n",
    "    return z \n",
    "\n",
    "im_idx = 1\n",
    "\n",
    "# Visualize the images\n",
    "def denormalize(x, mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD):\n",
    "    return x * torch.tensor(std).view(3, 1, 1) + torch.tensor(mean).view(3, 1, 1)\n",
    "\n",
    "# Visualize the raw image\n",
    "original_image = denormalize(images[im_idx].cpu()).permute(1, 2, 0).numpy()\n",
    "\n",
    "# Visualize the ground-truth latent\n",
    "# we first decode the latent\n",
    "with torch.no_grad():\n",
    "    decoded_image = vae.decode(sample_posterior(vaes)[im_idx].unsqueeze(0)).sample\n",
    "decoded_image = (decoded_image + 1.0) / 2.0\n",
    "decoded_image = decoded_image.clamp(0.0, 1.0).squeeze(0).cpu()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].imshow(original_image)\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(decoded_image.permute(1, 2, 0).numpy())\n",
    "ax[1].set_title(\"Reconstructed Ground-Truth Latent\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DINOv2 feature\n",
    "zs = []\n",
    "for encoder, encoder_type, arch in zip(encoders, encoder_types, architectures):\n",
    "    z = encoder.forward_features(images)['x_norm_patchtokens']\n",
    "    zs.append(z)\n",
    "\n",
    "print(zs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### See the diffusion loss part...\n",
    "\n",
    "def inpterpolant(t):\n",
    "    alpha_t = 1 - t\n",
    "    sigma_t = t\n",
    "    d_alpha_t = -1\n",
    "    d_sigma_t = 1\n",
    "    return alpha_t, sigma_t, d_alpha_t, d_sigma_t\n",
    "\n",
    "latents_scale = torch.tensor(\n",
    "    [0.18215, 0.18215, 0.18215, 0.18215]\n",
    "    ).view(1, 4, 1, 1).to(\"cuda:0\")\n",
    "latents_bias = torch.tensor(\n",
    "    [0., 0., 0., 0.]\n",
    "    ).view(1, 4, 1, 1).to(\"cuda:0\")\n",
    "\n",
    "x = sample_posterior(vaes, latents_scale=latents_scale, latents_bias=latents_bias)\n",
    "print(f\"vae: {vaes.shape}, x: {x.shape}\")\n",
    "\n",
    "model_kwargs = dict(y = labels.to(\"cuda:0\"))\n",
    "time_input = torch.rand((x.shape[0], 1, 1, 1), device=\"cuda:0\", dtype=x.dtype)\n",
    "print(time_input.shape)\n",
    "\n",
    "noises = torch.randn_like(x)\n",
    "# Linear interpolation\n",
    "alpha_t, sigma_t, d_alpha_t, d_sigma_t = inpterpolant(time_input)\n",
    "\n",
    "# Get the noisy latent\n",
    "model_input = alpha_t * x + sigma_t * noises\n",
    "# Get the denosing target\n",
    "model_target = d_alpha_t * x + d_sigma_t * noises\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output, zs_tilde = model(model_input, time_input.flatten(), **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the loss, and compare the patch-patch similarity...\n",
    "\n",
    "def mean_flat(x):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return torch.mean(x, dim=list(range(1, len(x.size()))))\n",
    "\n",
    "denoising_loss = mean_flat((model_output - model_target) ** 2)\n",
    "print(f\"Loss: {denoising_loss.cpu().detach().numpy()}\")\n",
    "\n",
    "\n",
    "proj_loss = []\n",
    "bsz = zs[0].shape[0]\n",
    "for i, (z, z_tilde) in enumerate(zip(zs, zs_tilde)):\n",
    "    inner_proj_loss = []\n",
    "    for j, (z_j, z_tilde_j) in enumerate(zip(z, z_tilde)):\n",
    "        z_tilde_j = torch.nn.functional.normalize(z_tilde_j, dim=-1) \n",
    "        z_j = torch.nn.functional.normalize(z_j, dim=-1) \n",
    "        inner_proj_loss.append(mean_flat((z_j * z_tilde_j).sum(dim=-1)).item())\n",
    "    proj_loss.append(inner_proj_loss)\n",
    "proj_loss = np.array(proj_loss).mean(axis=0)  # (column-wise) mean, get the cos-sim for each data point\n",
    "print(f\"Patch-Patch cos-sim: {proj_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
