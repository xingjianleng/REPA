{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/SSD/lengx/REPA\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.sit import SiT_models\n",
    "from models.original_sit import SiT_models as original_SiT_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sit_repa = torch.load(\"pretrained_models/last.pt\", map_location=torch.device('cpu'))\n",
    "sit = torch.load(\"pretrained_models/SiT-XL-2-256.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_kwargs = {\"fused_attn\": False, \"qk_norm\": False}\n",
    "sit_xl = SiT_models['SiT-XL/2'](\n",
    "    latent_size=32,\n",
    "    num_classes=1000,\n",
    "    use_cfg=True,\n",
    "    z_dims=[768],\n",
    "    encoder_depth=8,\n",
    "    **block_kwargs,\n",
    ")\n",
    "sit_xl.load_state_dict(sit_repa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_sit_xl = original_SiT_models['SiT-XL/2'](\n",
    "    input_size=32,\n",
    "    num_classes=1000,\n",
    ")\n",
    "original_sit_xl.load_state_dict(sit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "{'blocks.4.attn.proj.weight', 'blocks.13.attn.qkv.weight', 'blocks.20.mlp.fc1.bias', 'blocks.23.mlp.fc2.weight', 'blocks.24.attn.qkv.bias', 'blocks.0.mlp.fc2.weight', 'blocks.26.mlp.fc2.bias', 'y_embedder.embedding_table.weight', 'blocks.27.attn.proj.bias', 'blocks.23.mlp.fc1.weight', 'blocks.6.attn.proj.bias', 'blocks.0.adaLN_modulation.1.weight', 'blocks.20.adaLN_modulation.1.weight', 'blocks.24.adaLN_modulation.1.weight', 'blocks.6.mlp.fc1.weight', 'blocks.24.mlp.fc2.bias', 'blocks.16.mlp.fc1.weight', 'blocks.5.mlp.fc1.weight', 'blocks.17.adaLN_modulation.1.weight', 'blocks.2.attn.qkv.weight', 'blocks.6.mlp.fc2.bias', 'blocks.14.mlp.fc1.weight', 'blocks.8.mlp.fc1.weight', 'blocks.22.mlp.fc2.bias', 'blocks.24.attn.proj.bias', 'blocks.22.adaLN_modulation.1.weight', 'blocks.23.attn.qkv.weight', 'blocks.9.mlp.fc1.bias', 'blocks.27.adaLN_modulation.1.weight', 'blocks.19.mlp.fc1.bias', 'blocks.0.mlp.fc1.weight', 'final_layer.linear.weight', 'blocks.18.attn.proj.bias', 'blocks.0.attn.proj.bias', 'blocks.20.attn.proj.bias', 'blocks.14.mlp.fc2.bias', 'blocks.15.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.22.mlp.fc2.weight', 'blocks.25.attn.proj.bias', 'blocks.7.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.21.attn.qkv.bias', 'blocks.12.mlp.fc2.bias', 'blocks.12.attn.proj.weight', 'blocks.21.mlp.fc1.weight', 'blocks.6.attn.proj.weight', 'blocks.11.mlp.fc1.weight', 'blocks.5.adaLN_modulation.1.weight', 'blocks.15.mlp.fc2.weight', 'blocks.8.mlp.fc2.weight', 'blocks.13.adaLN_modulation.1.weight', 'blocks.3.attn.proj.weight', 'blocks.23.attn.proj.bias', 'blocks.23.attn.proj.weight', 'blocks.8.attn.qkv.weight', 'blocks.9.adaLN_modulation.1.weight', 'blocks.17.adaLN_modulation.1.bias', 'blocks.19.attn.qkv.weight', 'blocks.26.mlp.fc1.bias', 'blocks.12.adaLN_modulation.1.bias', 'blocks.6.mlp.fc1.bias', 'blocks.9.mlp.fc2.bias', 't_embedder.mlp.2.bias', 'blocks.18.mlp.fc2.bias', 'blocks.0.attn.proj.weight', 'blocks.3.mlp.fc2.weight', 'blocks.26.attn.proj.bias', 'blocks.20.mlp.fc2.bias', 'blocks.9.attn.qkv.bias', 'blocks.25.attn.proj.weight', 'blocks.9.mlp.fc2.weight', 'blocks.14.adaLN_modulation.1.bias', 'blocks.18.mlp.fc1.bias', 'blocks.4.mlp.fc1.bias', 'blocks.20.adaLN_modulation.1.bias', 'projectors.0.4.weight', 'blocks.26.mlp.fc2.weight', 'blocks.12.mlp.fc2.weight', 'blocks.5.adaLN_modulation.1.bias', 'blocks.17.attn.proj.weight', 'blocks.16.attn.qkv.bias', 'blocks.25.mlp.fc1.bias', 'blocks.8.mlp.fc1.bias', 'blocks.3.mlp.fc1.weight', 'blocks.23.adaLN_modulation.1.weight', 'blocks.25.adaLN_modulation.1.weight', 'blocks.18.adaLN_modulation.1.bias', 'blocks.7.attn.qkv.weight', 'blocks.12.mlp.fc1.weight', 'blocks.7.attn.proj.weight', 'blocks.20.attn.qkv.bias', 'projectors.0.2.weight', 'blocks.10.attn.qkv.weight', 'blocks.0.attn.qkv.weight', 'blocks.10.attn.proj.bias', 'blocks.15.mlp.fc1.bias', 'blocks.16.attn.proj.bias', 'blocks.11.attn.proj.weight', 'blocks.19.attn.qkv.bias', 'blocks.10.adaLN_modulation.1.bias', 'blocks.11.adaLN_modulation.1.bias', 'blocks.13.mlp.fc1.bias', 'blocks.11.attn.proj.bias', 'blocks.17.mlp.fc1.bias', 'blocks.25.mlp.fc2.weight', 'blocks.26.mlp.fc1.weight', 'blocks.5.attn.qkv.bias', 'blocks.16.attn.proj.weight', 'blocks.23.mlp.fc2.bias', 'blocks.17.mlp.fc2.bias', 'blocks.8.adaLN_modulation.1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.27.attn.proj.weight', 'blocks.0.mlp.fc2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.9.adaLN_modulation.1.bias', 'blocks.17.attn.qkv.bias', 'blocks.4.adaLN_modulation.1.weight', 'blocks.16.adaLN_modulation.1.weight', 't_embedder.mlp.0.weight', 'blocks.11.adaLN_modulation.1.weight', 'blocks.16.mlp.fc1.bias', 'blocks.17.mlp.fc1.weight', 'blocks.21.mlp.fc1.bias', 'blocks.27.mlp.fc2.bias', 'blocks.14.attn.proj.bias', 'blocks.21.adaLN_modulation.1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.17.attn.qkv.weight', 'blocks.17.attn.proj.bias', 'blocks.21.attn.proj.weight', 'blocks.13.adaLN_modulation.1.bias', 'blocks.14.mlp.fc2.weight', 'blocks.0.attn.qkv.bias', 'blocks.22.mlp.fc1.bias', 'blocks.0.adaLN_modulation.1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.25.mlp.fc2.bias', 'blocks.15.adaLN_modulation.1.bias', 'blocks.24.mlp.fc1.bias', 'blocks.27.attn.qkv.weight', 'blocks.2.mlp.fc2.bias', 'blocks.7.attn.qkv.bias', 'blocks.22.mlp.fc1.weight', 'blocks.24.attn.qkv.weight', 'blocks.27.attn.qkv.bias', 'blocks.1.attn.qkv.weight', 'blocks.6.attn.qkv.weight', 'blocks.1.mlp.fc2.bias', 'blocks.9.attn.qkv.weight', 'blocks.18.mlp.fc2.weight', 'blocks.2.mlp.fc1.weight', 'blocks.19.mlp.fc1.weight', 'blocks.27.mlp.fc1.bias', 'blocks.13.mlp.fc2.bias', 'blocks.22.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.27.adaLN_modulation.1.bias', 'blocks.22.attn.qkv.weight', 'blocks.25.mlp.fc1.weight', 'blocks.15.attn.qkv.weight', 'blocks.17.mlp.fc2.weight', 'blocks.15.mlp.fc2.bias', 'blocks.11.attn.qkv.weight', 'blocks.3.adaLN_modulation.1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.13.attn.proj.weight', 'blocks.26.attn.qkv.weight', 'blocks.20.attn.qkv.weight', 'blocks.24.adaLN_modulation.1.bias', 'blocks.22.adaLN_modulation.1.bias', 'blocks.3.adaLN_modulation.1.bias', 'blocks.15.attn.proj.bias', 'blocks.25.adaLN_modulation.1.bias', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.bias', 'blocks.3.attn.proj.bias', 'blocks.16.mlp.fc2.weight', 'blocks.22.attn.proj.weight', 'blocks.12.attn.proj.bias', 'blocks.18.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.10.mlp.fc2.bias', 'blocks.19.attn.proj.bias', 'blocks.6.mlp.fc2.weight', 'blocks.21.adaLN_modulation.1.bias', 'blocks.4.attn.qkv.weight', 'projectors.0.0.bias', 'blocks.11.mlp.fc2.weight', 'blocks.2.adaLN_modulation.1.bias', 'blocks.10.adaLN_modulation.1.weight', 'blocks.13.mlp.fc2.weight', 'blocks.8.attn.proj.weight', 'blocks.8.adaLN_modulation.1.weight', 'blocks.10.mlp.fc1.weight', 'blocks.20.attn.proj.weight', 'blocks.21.attn.proj.bias', 'blocks.21.mlp.fc2.bias', 'blocks.23.mlp.fc1.bias', 'blocks.3.mlp.fc2.bias', 'blocks.19.mlp.fc2.weight', 'blocks.23.attn.qkv.bias', 'blocks.26.adaLN_modulation.1.weight', 'blocks.6.adaLN_modulation.1.weight', 'final_layer.linear.bias', 'blocks.5.attn.qkv.weight', 'blocks.3.mlp.fc1.bias', 'blocks.12.attn.qkv.weight', 'blocks.12.attn.qkv.bias', 'blocks.16.adaLN_modulation.1.bias', 'blocks.1.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.1.attn.proj.weight', 'blocks.6.adaLN_modulation.1.bias', 'blocks.13.mlp.fc1.weight', 'blocks.13.attn.qkv.bias', 'blocks.11.mlp.fc1.bias', 'blocks.7.attn.proj.bias', 'blocks.18.attn.proj.weight', 'blocks.18.adaLN_modulation.1.weight', 'blocks.9.attn.proj.bias', 'blocks.18.mlp.fc1.weight', 'blocks.22.attn.proj.bias', 'pos_embed', 'blocks.2.attn.proj.weight', 'blocks.21.attn.qkv.weight', 'projectors.0.2.bias', 'blocks.19.mlp.fc2.bias', 'x_embedder.proj.bias', 'blocks.11.mlp.fc2.bias', 't_embedder.mlp.2.weight', 'final_layer.adaLN_modulation.1.weight', 'blocks.14.attn.proj.weight', 'blocks.2.adaLN_modulation.1.weight', 'blocks.27.mlp.fc2.weight', 'blocks.9.mlp.fc1.weight', 'blocks.21.mlp.fc2.weight', 'blocks.23.adaLN_modulation.1.bias', 'blocks.24.mlp.fc2.weight', 'blocks.1.adaLN_modulation.1.weight', 'blocks.7.adaLN_modulation.1.weight', 'blocks.14.attn.qkv.bias', 'blocks.20.mlp.fc1.weight', 'blocks.10.attn.proj.weight', 'blocks.12.mlp.fc1.bias', 'blocks.19.attn.proj.weight', 'blocks.15.mlp.fc1.weight', 'blocks.1.adaLN_modulation.1.bias', 'blocks.4.attn.qkv.bias', 'blocks.15.adaLN_modulation.1.weight', 'blocks.24.attn.proj.weight', 't_embedder.mlp.0.bias', 'blocks.4.attn.proj.bias', 'blocks.7.adaLN_modulation.1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.26.attn.proj.weight', 'blocks.7.mlp.fc2.bias', 'blocks.5.mlp.fc2.bias', 'blocks.12.adaLN_modulation.1.weight', 'blocks.6.attn.qkv.bias', 'blocks.8.mlp.fc2.bias', 'blocks.24.mlp.fc1.weight', 'projectors.0.0.weight', 'blocks.10.attn.qkv.bias', 'blocks.2.mlp.fc1.bias', 'blocks.19.adaLN_modulation.1.weight', 'blocks.15.attn.qkv.bias', 'blocks.14.mlp.fc1.bias', 'blocks.16.mlp.fc2.bias', 'blocks.4.adaLN_modulation.1.bias', 'x_embedder.proj.weight', 'blocks.11.attn.qkv.bias', 'blocks.3.attn.qkv.weight', 'blocks.18.attn.qkv.weight', 'blocks.2.attn.proj.bias', 'blocks.4.mlp.fc2.bias', 'blocks.14.adaLN_modulation.1.weight', 'blocks.16.attn.qkv.weight', 'blocks.25.attn.qkv.bias', 'blocks.7.mlp.fc1.weight', 'blocks.5.attn.proj.bias', 'blocks.3.attn.qkv.bias', 'blocks.27.mlp.fc1.weight', 'blocks.13.attn.proj.bias', 'blocks.20.mlp.fc2.weight', 'blocks.2.attn.qkv.bias', 'final_layer.adaLN_modulation.1.bias', 'blocks.0.mlp.fc1.bias', 'blocks.8.attn.qkv.bias', 'blocks.4.mlp.fc2.weight', 'projectors.0.4.bias', 'blocks.19.adaLN_modulation.1.bias', 'blocks.26.adaLN_modulation.1.bias', 'blocks.14.attn.qkv.weight', 'blocks.25.attn.qkv.weight', 'blocks.7.mlp.fc2.weight', 'blocks.26.attn.qkv.bias'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(set(sit.keys()) - set(sit_repa.keys()))\n",
    "print(set(sit_repa.keys()) & set(sit.keys()))\n",
    "print(set(sit_repa.keys()) - set(sit.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in set(sit_repa.keys()) & set(sit.keys()):\n",
    "    if sit[elem].shape != sit_repa[elem].shape:\n",
    "        print(f\"SiT: {elem} - {sit[elem].shape} === SiT-RPEA: {elem} - {sit_repa[elem].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in (set(sit_repa.keys()) - set(sit.keys())):\n",
    "    sit[elem] = torch.rand_like(sit_repa[elem])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_layer.linear.weight torch.Size([32, 1152])\n",
      "torch.Size([16, 1152])\n",
      "final_layer.linear.bias torch.Size([32])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "for k, v in sit.items():\n",
    "    if \"final_layer.linear\" in k:\n",
    "        # sit[k] = v.chunk(2, dim=0)[0]\n",
    "        print(k, v.shape)\n",
    "        print(v.chunk(2, dim=0)[0].shape)\n",
    "        lst.append(k)\n",
    "\n",
    "for k in lst:\n",
    "    sit[k] = sit[k].chunk(2, dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sit, \"pretrained_models/SiT-XL-2-256-fixed.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/lengx/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/lengx/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/lengx/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/lengx/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1536, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-39): 40 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SwiGLUFFNFused(\n",
       "        (w12): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (w3): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### From above, we know that the original SiT implementation is different from the REPA version, sigma prediction etc.\n",
    "### Let's investigate more about the original SiT features, and its alignment with the DINOv2 features\n",
    "\n",
    "### Improts\n",
    "import torch\n",
    "import random\n",
    "import timm\n",
    "from models.sit import SiT_models\n",
    "from diffusers import AutoencoderKL\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import PIL\n",
    "import json\n",
    "import gc\n",
    "import io\n",
    "from torchvision.transforms import Normalize\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from metrics import AlignmentMetrics\n",
    "\n",
    "pyspng = None\n",
    "\n",
    "### Helper funcs\n",
    "def load_h5_file(hf, path):\n",
    "    # Helper function to load files from h5 file\n",
    "    if path.endswith('.png'):\n",
    "        if pyspng is not None:\n",
    "            rtn = pyspng.load(io.BytesIO(np.array(hf[path])))\n",
    "        else:\n",
    "            rtn = np.array(PIL.Image.open(io.BytesIO(np.array(hf[path]))))\n",
    "        rtn = rtn.reshape(*rtn.shape[:2], -1).transpose(2, 0, 1)\n",
    "    elif path.endswith('.json'):\n",
    "        rtn = json.loads(np.array(hf[path]).tobytes().decode('utf-8'))\n",
    "    elif path.endswith('.npy'):\n",
    "        rtn= np.array(hf[path])\n",
    "    else:\n",
    "        raise ValueError('Unknown file type: {}'.format(path))\n",
    "    return rtn\n",
    "\n",
    "def preprocess_raw_image(x, enc_type):\n",
    "    x = x / 255.\n",
    "    x = Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)(x)\n",
    "    x = torch.nn.functional.interpolate(x, 224, mode='bicubic')\n",
    "    x = torch.nn.functional.interpolate(x, 224, mode='bicubic')\n",
    "    return x\n",
    "\n",
    "def interpolant(t):\n",
    "    alpha_t = 1 - t\n",
    "    sigma_t = t\n",
    "    d_alpha_t = -1\n",
    "    d_sigma_t = 1\n",
    "    return alpha_t, sigma_t, d_alpha_t, d_sigma_t\n",
    "\n",
    "def mean_flat(x):\n",
    "    return torch.mean(x, dim=list(range(1, len(x.size()))))\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_posterior(moments, latents_scale=1., latents_bias=0.):\n",
    "    mean, std = torch.chunk(moments, 2, dim=1)\n",
    "    z = mean + std * torch.randn_like(mean)\n",
    "    z = (z * latents_scale + latents_bias) \n",
    "    return z\n",
    "\n",
    "latents_scale = torch.tensor(\n",
    "    [0.18215, 0.18215, 0.18215, 0.18215]\n",
    ").view(1, 4, 1, 1).to(\"cuda:0\")\n",
    "latents_bias = torch.tensor(\n",
    "    [0., 0., 0., 0.]\n",
    ").view(1, 4, 1, 1).to(\"cuda:0\")\n",
    "\n",
    "### Sample data\n",
    "with open(\"data/images_h5.json\", \"r\") as f:\n",
    "    images_h5_cfg = json.load(f)\n",
    "with open(\"data/vae-sd_h5.json\", \"r\") as f:\n",
    "    vae_h5_cfg = json.load(f)\n",
    "\n",
    "N = 256\n",
    "BS = 8\n",
    "chosen_files = random.Random(42).sample(images_h5_cfg, N)\n",
    "chosen_vaes = [elem.replace(\"img\", \"img-mean-std-\").replace(\".png\", \".npy\") for elem in chosen_files]\n",
    "\n",
    "image_h5 = h5py.File(\"data/images.h5\", \"r\")\n",
    "vae_h5 = h5py.File(\"data/vae-sd.h5\", \"r\")\n",
    "\n",
    "### Labels...\n",
    "fname = 'dataset.json'\n",
    "labels = load_h5_file(vae_h5, fname)['labels']\n",
    "labels = dict(labels)\n",
    "labels = [labels[fname.replace('\\\\', '/')] for fname in chosen_vaes]\n",
    "labels = np.array(labels)\n",
    "labels = labels.astype({1: np.int64, 2: np.float32}[labels.ndim])\n",
    "\n",
    "images = preprocess_raw_image(torch.stack([torch.from_numpy(load_h5_file(image_h5, elem)) for elem in chosen_files]), \"dinov2-vit-b\")\n",
    "vaes = torch.stack([torch.from_numpy(load_h5_file(vae_h5, elem)) for elem in chosen_vaes])\n",
    "labels = torch.from_numpy(labels)\n",
    "\n",
    "\n",
    "### Prepare the external encoder\n",
    "encoder = torch.hub.load('facebookresearch/dinov2', f'dinov2_vitg14')\n",
    "del encoder.head\n",
    "encoder.pos_embed.data = timm.layers.pos_embed.resample_abs_pos_embed(\n",
    "    encoder.pos_embed.data, [16, 16],\n",
    ")\n",
    "encoder.head = torch.nn.Identity()\n",
    "encoder = encoder.to(\"cuda:0\")\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
