{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from utils import load_encoders\n",
    "from models.sit import SiT_models\n",
    "from diffusers import AutoencoderKL\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import PIL\n",
    "import json\n",
    "import gc\n",
    "import io\n",
    "from torchvision.transforms import Normalize\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from metrics import AlignmentMetrics\n",
    "\n",
    "pyspng = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the data\n",
    "with open(\"data/images_h5.json\", \"r\") as f:\n",
    "    images_h5_cfg = json.load(f)\n",
    "with open(\"data/vae-sd_h5.json\", \"r\") as f:\n",
    "    vae_h5_cfg = json.load(f)\n",
    "\n",
    "def load_h5_file(hf, path):\n",
    "    # Helper function to load files from h5 file\n",
    "    if path.endswith('.png'):\n",
    "        if pyspng is not None:\n",
    "            rtn = pyspng.load(io.BytesIO(np.array(hf[path])))\n",
    "        else:\n",
    "            rtn = np.array(PIL.Image.open(io.BytesIO(np.array(hf[path]))))\n",
    "        rtn = rtn.reshape(*rtn.shape[:2], -1).transpose(2, 0, 1)\n",
    "    elif path.endswith('.json'):\n",
    "        rtn = json.loads(np.array(hf[path]).tobytes().decode('utf-8'))\n",
    "    elif path.endswith('.npy'):\n",
    "        rtn= np.array(hf[path])\n",
    "    else:\n",
    "        raise ValueError('Unknown file type: {}'.format(path))\n",
    "    return rtn\n",
    "\n",
    "def preprocess_raw_image(x, enc_type):\n",
    "    x = x / 255.\n",
    "    x = Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)(x)\n",
    "    x = torch.nn.functional.interpolate(x, 224, mode='bicubic')\n",
    "    x = torch.nn.functional.interpolate(x, 224, mode='bicubic')\n",
    "    return x\n",
    "\n",
    "# N = 256\n",
    "N = 64\n",
    "BS = 8\n",
    "chosen_files = random.Random(42).sample(images_h5_cfg, N)\n",
    "chosen_vaes = [elem.replace(\"img\", \"img-mean-std-\").replace(\".png\", \".npy\") for elem in chosen_files]\n",
    "\n",
    "image_h5 = h5py.File(\"data/images.h5\", \"r\")\n",
    "vae_h5 = h5py.File(\"data/vae-sd.h5\", \"r\")\n",
    "\n",
    "### Labels...\n",
    "fname = 'dataset.json'\n",
    "labels = load_h5_file(vae_h5, fname)['labels']\n",
    "labels = dict(labels)\n",
    "labels = [labels[fname.replace('\\\\', '/')] for fname in chosen_vaes]\n",
    "labels = np.array(labels)\n",
    "labels = labels.astype({1: np.int64, 2: np.float32}[labels.ndim])\n",
    "\n",
    "images = preprocess_raw_image(torch.stack([torch.from_numpy(load_h5_file(image_h5, elem)) for elem in chosen_files]), \"dinov2-vit-b\")\n",
    "vaes = torch.stack([torch.from_numpy(load_h5_file(vae_h5, elem)) for elem in chosen_vaes])\n",
    "labels = torch.from_numpy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load external visual model, and extract features\n",
    "encoders, encoder_types, architectures = load_encoders(\"dinov2-vit-b\", \"cuda:0\")\n",
    "# encoders, encoder_types, architectures = load_encoders(\"dinov2-vit-l\", \"cuda:0\")\n",
    "# encoders, encoder_types, architectures = load_encoders(\"dinov2-vit-g\", \"cuda:0\")\n",
    "\n",
    "encoder, encoder_type, architecture = encoders[0], encoder_types[0], architectures[0]\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to(\"cuda:0\")\n",
    "\n",
    "zs = []\n",
    "for i in tqdm(range(0, N, BS)):\n",
    "    images_batch = preprocess_raw_image(images[i:i+BS].to(\"cuda:0\"), encoder_type)\n",
    "    with torch.no_grad():\n",
    "        z = encoder.forward_features(images_batch)['x_norm_patchtokens']\n",
    "        zs.append(z)\n",
    "zs = torch.cat(zs, dim=0).cpu()\n",
    "\n",
    "\n",
    "# # NOTE: The model checkpoints we are interested in\n",
    "# ckpt_paths = [\n",
    "#     \"exps/sit-b-base-400k/checkpoints\", \"exps/sit-b-linear-dinov2-b-enc8-400k/checkpoints\",\n",
    "#     # \"exps/sit-b-linear-dinov2-b-enc8-patch-0.5-400k/checkpoints\", \"exps/sit-b-linear-dinov2-b-enc8-patch-0.75-400k/checkpoints\",\n",
    "# ]\n",
    "# model_name = \"SiT-B/2\"\n",
    "\n",
    "# ckpt_paths = [\"exps/sit-xl-base-400k/checkpoints\", \"exps/sit-xl-linear-dinov2-b-enc8-400k/checkpoints\"]\n",
    "# model_name = \"SiT-XL/2\"\n",
    "\n",
    "ckpt_paths = [\"pretrained_models\"]\n",
    "model_name = \"SiT-XL/2\"\n",
    "\n",
    "\n",
    "### Model HPs\n",
    "resolution = 256\n",
    "num_classes = 1000\n",
    "assert resolution % 8 == 0\n",
    "latent_size = resolution // 8\n",
    "# z_dims = [encoder.embed_dim for encoder in encoders]\n",
    "z_dims = [768]\n",
    "encoder_depth = 8\n",
    "block_kwargs = {\"fused_attn\": False, \"qk_norm\": False}\n",
    "\n",
    "\n",
    "def get_model(ckpt_path):\n",
    "    model = SiT_models[model_name](\n",
    "        latent_size=latent_size,\n",
    "        num_classes=num_classes,\n",
    "        use_cfg=True,\n",
    "        z_dims=z_dims,\n",
    "        encoder_depth=encoder_depth,\n",
    "        **block_kwargs,\n",
    "    ).to(\"cuda:0\")\n",
    "    state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    if \"model\" in state_dict:\n",
    "        state_dict = state_dict[\"model\"]\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def interpolant(t):\n",
    "    alpha_t = 1 - t\n",
    "    sigma_t = t\n",
    "    d_alpha_t = -1\n",
    "    d_sigma_t = 1\n",
    "    return alpha_t, sigma_t, d_alpha_t, d_sigma_t\n",
    "\n",
    "def mean_flat(x):\n",
    "    return torch.mean(x, dim=list(range(1, len(x.size()))))\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_posterior(moments, latents_scale=1., latents_bias=0.):\n",
    "    mean, std = torch.chunk(moments, 2, dim=1)\n",
    "    z = mean + std * torch.randn_like(mean)\n",
    "    z = (z * latents_scale + latents_bias) \n",
    "    return z\n",
    "\n",
    "latents_scale = torch.tensor(\n",
    "    [0.18215, 0.18215, 0.18215, 0.18215]\n",
    ").view(1, 4, 1, 1).to(\"cuda:0\")\n",
    "latents_bias = torch.tensor(\n",
    "    [0., 0., 0., 0.]\n",
    ").view(1, 4, 1, 1).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_stats = {}\n",
    "\n",
    "for ckpt_path in ckpt_paths:\n",
    "    # stats_neg_cos_sim = {}\n",
    "    # stats_frobenius_l2_norm = {}\n",
    "    stats_cknna_align = {}\n",
    "    stats_mutual_knn = {}\n",
    "    stats_p2p_align = {}\n",
    "\n",
    "    for elem in sorted(os.listdir(ckpt_path)):\n",
    "        # Skip raw SiT checkpoints\n",
    "        if elem == \"SiT-XL-2-256.pt\": continue\n",
    "        model = get_model(os.path.join(ckpt_path, elem))\n",
    "\n",
    "        # Call gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        zs_tilde = []\n",
    "        for i in tqdm(range(0, N, BS)):\n",
    "            vaes_batch = vaes[i:i+BS].to(\"cuda:0\")\n",
    "            labels_batch = labels[i:i+BS].to(\"cuda:0\")\n",
    "            x = sample_posterior(vaes_batch, latents_scale=latents_scale, latents_bias=latents_bias)\n",
    "            model_kwargs = dict(y = labels_batch)\n",
    "            # time_input = torch.rand((x.shape[0], 1, 1, 1), device=\"cuda:0\", dtype=x.dtype)\n",
    "            time_input = torch.rand((x.shape[0], 1, 1, 1), device=\"cuda:0\", dtype=x.dtype)\n",
    "            # time_input = torch.fill_(torch.zeros((x.shape[0], 1, 1, 1), device=\"cuda:0\", dtype=x.dtype), 0.5)  # <--- control on time\n",
    "\n",
    "            noises = torch.randn_like(x)\n",
    "            alpha_t, sigma_t, d_alpha_t, d_sigma_t = interpolant(time_input)  # linear\n",
    "\n",
    "            model_input = alpha_t * x + sigma_t * noises\n",
    "            model_target = d_alpha_t * x + d_sigma_t * noises\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model_output, z_tilde = model.forward_features(model_input, time_input.flatten(), **model_kwargs)\n",
    "                # zs_tilde.append(z_tilde[0])\n",
    "                zs_tilde.append(z_tilde)\n",
    "        zs_tilde = torch.cat(zs_tilde, dim=0)\n",
    "\n",
    "        cknna_align = []\n",
    "        mutual_knn_align = []\n",
    "        p2p_align = []\n",
    "        for l in range(zs_tilde.shape[1]):\n",
    "            # CKNNA method for alignment\n",
    "            # print(zs_tilde[:, l].mean(dim=1).shape)\n",
    "            # print(zs.mean(dim=1).shape)\n",
    "            # break\n",
    "            cknna = AlignmentMetrics.cknna(zs.mean(dim=1), zs_tilde[:, l].mean(dim=1), topk=10)\n",
    "            mutual_knn = AlignmentMetrics.mutual_knn(zs.mean(dim=1), zs_tilde[:, l].mean(dim=1), topk=10)\n",
    "            p2p = AlignmentMetrics.patch2patch_kernel_alignment_score(zs, zs_tilde[:, l])\n",
    "            cknna_align.append(cknna)\n",
    "            mutual_knn_align.append(mutual_knn)\n",
    "            p2p_align.append(p2p)\n",
    "\n",
    "        step = int(elem[:-3]) if elem != \"last.pt\" and elem != \"SiT-XL-2-256_fixed.pt\" else 7_000_000\n",
    "        stats_cknna_align[step] = cknna_align\n",
    "        stats_mutual_knn[step] =  mutual_knn_align\n",
    "        stats_p2p_align[step] = p2p_align\n",
    "\n",
    "    ckpt_stats[ckpt_path] = {\"cknna_align\": stats_cknna_align, \"mutual_knn\": stats_mutual_knn, \"p2p_align\": stats_p2p_align}\n",
    "    # ckpt_stats[ckpt_path] = {\"neg_cos_sim\": stats_neg_cos_sim, \"kernel_align_patch_l2\": stats_frobenius_l2_norm, \"cknna_align\": stats_cknna_align, \"mutual_knn\": stats_mutual_knn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_stats_path = \"ckpt_stats_sit_b.json\"\n",
    "ckpt_stats_path = \"ckpt_stats_sit_xl.json\"\n",
    "\n",
    "if os.path.exists(ckpt_stats_path):\n",
    "    with open(ckpt_stats_path, \"r\") as f:\n",
    "        ckpt_stats = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt_stats.keys())\n",
    "print(ckpt_stats[ckpt_paths[0]].keys())\n",
    "print(ckpt_stats[ckpt_paths[0]][\"cknna_align\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_paths[0] == \"pretrained_models\":\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(np.array(range(len(cknna_align))) + 1, cknna_align, label=\"CKNNA\",marker='o')\n",
    "    plt.plot(np.array(range(len(cknna_align))) + 1, mutual_knn_align, label=\"Mutual KNN\", marker='x')\n",
    "    plt.plot(np.array(range(len(cknna_align))) + 1, p2p_align, label=\"Patch2Patch\", marker='s')\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(\"Alignment score\")\n",
    "    plt.title(\"Alignment metrics for SiT-XL/2 model @ 7M steps\")\n",
    "    plt.xticks(np.array(range(len(cknna_align))) + 1)\n",
    "    plt.ylim(0., 1.0)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 plots for Figure 2 in the paper\n",
    "\n",
    "# k = \"exps/sit-b-base-400k/checkpoints\"\n",
    "# k = \"exps/sit-b-linear-dinov2-b-enc8-400k/checkpoints\"\n",
    "k = 'exps/sit-xl-base-400k/checkpoints'\n",
    "# k = 'exps/sit-xl-linear-dinov2-b-enc8-400k/checkpoints'\n",
    "\n",
    "metrics = \"cknna_align\"\n",
    "# metrics = \"p2p_align\"\n",
    "\n",
    "if k in ckpt_paths:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for step, align in ckpt_stats[k][metrics].items():\n",
    "        plt.plot(np.array(range(len(align))) + 1, align, label=f\"Step: {step}\", marker='o')\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(f\"Alignment score ({metrics})\")\n",
    "    plt.title(k)\n",
    "    plt.xticks(np.array(range(len(align))) + 1)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# if \"exps/sit-b-linear-dinov2-b-enc8-400k/checkpoints\" in ckpt_paths:\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     for step, align in ckpt_stats[\"exps/sit-b-linear-dinov2-b-enc8-400k/checkpoints\"][\"cknna_align\"].items():\n",
    "#         plt.plot(np.array(range(len(align))) + 1, align, label=f\"Step: {step}\", marker='o')\n",
    "#     plt.xlabel(\"Layer\")\n",
    "#     plt.ylabel(\"Alignment score (CKNNA)\")\n",
    "#     plt.title(\"sit-b-linear-dinov2-b-enc8-400k\")\n",
    "#     plt.xticks(np.array(range(len(align))) + 1)\n",
    "#     plt.grid()\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trend plot, x -> steps, y -> alignment score of a specific layer (e.g, 8th layer)\n",
    "\n",
    "# k = \"exps/sit-b-base-400k/checkpoints\"\n",
    "# k = \"exps/sit-b-linear-dinov2-b-enc8-400k/checkpoints\"\n",
    "# k = 'exps/sit-xl-base-400k/checkpoints'\n",
    "k = 'exps/sit-xl-linear-dinov2-b-enc8-400k/checkpoints'\n",
    "\n",
    "enc_layer = 8\n",
    "# enc_layer = 11\n",
    "# enc_layer = -1\n",
    "\n",
    "metrics = \"cknna_align\"\n",
    "# metrics = \"p2p_align\"\n",
    "\n",
    "if k in ckpt_paths:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    steps = []\n",
    "    aligns = []\n",
    "    for step, align in ckpt_stats[k][metrics].items():\n",
    "        steps.append(step)\n",
    "        if enc_layer == -1:\n",
    "            aligns.append(np.max(align))\n",
    "        else:\n",
    "            aligns.append(align[enc_layer - 1])\n",
    "    plt.plot(steps, aligns, label=\"Max\" if enc_layer == -1 else f\"Layer: {enc_layer}\", marker='o')\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(f\"Alignment score ({metrics})\")\n",
    "    plt.title(k)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ckpt_stats_path):\n",
    "    with open(ckpt_stats_path, \"w\") as f:\n",
    "        json.dump(ckpt_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
